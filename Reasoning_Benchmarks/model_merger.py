# æ–‡ä»¶å: model_merger.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os
from tqdm import tqdm
import gc
from collections import defaultdict
from typing import List,Optional

# Set the visible CUDA device
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

def save_model_and_tokenizer(model, tokenizer_name, output_path):
    """Saves the model and its corresponding tokenizer to the specified path."""
    print(f"\nEnsuring output directory exists: {output_path}")
    os.makedirs(output_path, exist_ok=True)
    print(f"Saving the merged model to: {output_path}")
    model.save_pretrained(output_path)
    print(f"Loading and saving the tokenizer from {tokenizer_name} to: {output_path}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)
        tokenizer.save_pretrained(output_path)
    except Exception as e:
        print(f"Could not save tokenizer. Error: {e}")
    print(f"Process for '{output_path}' completed successfully!")
def extract_magnitude_components(tensor, sparsity_threshold, component_ratio):
    """
    åŸºäºå¹…åº¦æå–ç¨€ç–ç»„ä»¶
    """
    # è®¡ç®—å¹…åº¦
    magnitudes = torch.abs(tensor)
    
    # è®¡ç®—é˜ˆå€¼
    num_params = tensor.numel()
    num_to_keep = int(num_params * component_ratio)
    
    if num_to_keep > 0:
        # æ‰¾åˆ°é˜ˆå€¼
        threshold = torch.kthvalue(magnitudes.flatten(), num_params - num_to_keep + 1).values
        
        # åˆ›å»ºæ©ç 
        mask = magnitudes >= threshold
        
        # åº”ç”¨ç¨€ç–æ€§é˜ˆå€¼
        mask = mask & (magnitudes >= sparsity_threshold)
        
        return tensor * mask
    else:
        return torch.zeros_like(tensor)

def extract_gradient_components(tensor, sparsity_threshold, component_ratio):
    """
    åŸºäºæ¢¯åº¦ä¿¡æ¯æå–ç¨€ç–ç»„ä»¶
    """
    # è®¡ç®—æ¢¯åº¦çš„è¿‘ä¼¼ï¼ˆä½¿ç”¨æ•°å€¼å·®åˆ†ï¼‰
    epsilon = 1e-6
    gradient_approx = torch.abs(tensor)
    
    # åº”ç”¨å¹…åº¦æå–é€»è¾‘
    return extract_magnitude_components(gradient_approx, sparsity_threshold, component_ratio)

def extract_activation_components(tensor, sparsity_threshold, component_ratio):
    """
    åŸºäºæ¿€æ´»æ¨¡å¼æå–ç¨€ç–ç»„ä»¶
    """
    # è®¡ç®—æ¿€æ´»å¼ºåº¦
    activation_strength = torch.abs(tensor)
    
    # åº”ç”¨éçº¿æ€§å˜æ¢æ¨¡æ‹Ÿæ¿€æ´»
    activation_strength = torch.tanh(activation_strength)
    
    # åº”ç”¨å¹…åº¦æå–é€»è¾‘
    return extract_magnitude_components(activation_strength, sparsity_threshold, component_ratio)

def sce_merge_models(base_model_name, model_names_list, sparsity_threshold, scaling_lambda,  output_path, fusion_weights=None,
                     extraction_method="magnitude", component_ratio=0.1):
    """
    ä½¿ç”¨ SCE (Sparse Component Extraction) æ–¹æ³•åˆå¹¶å¤šä¸ªæ¨¡å‹ã€‚
    SCE é€šè¿‡è¯†åˆ«å’Œæå–æ¨¡å‹ä¸­çš„ç¨€ç–ç»„ä»¶æ¥è¿›è¡Œåˆå¹¶ã€‚
    
    Args:
        base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_names_list (list[str]): éœ€è¦åˆå¹¶çš„å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„åˆ—è¡¨ã€‚
        sparsity_threshold (float): ç¨€ç–æ€§é˜ˆå€¼ï¼Œç”¨äºè¯†åˆ«ç¨€ç–ç»„ä»¶ã€‚
        scaling_lambda (float): ä»»åŠ¡ç®—æœ¯çš„ç¼©æ”¾ç³»æ•° Î»ã€‚
        output_path (str): åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚
        extraction_method (str): ç»„ä»¶æå–æ–¹æ³•ï¼Œæ”¯æŒ "magnitude", "gradient", "activation"ã€‚
        component_ratio (float): ä¿ç•™çš„ç»„ä»¶æ¯”ä¾‹ (0.0 åˆ° 1.0)ã€‚
        fusion_weights (list[float], optional): å„æ¨¡å‹çš„èåˆæƒé‡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç­‰æƒé‡ã€‚
    """
    print("-" * 80)
    print(f"Starting SCE-Merge with sparsity_threshold={sparsity_threshold} and scaling_lambda={scaling_lambda}")
    print(f"Extraction method: {extraction_method}, Component ratio: {component_ratio}")
    
    # --- å¤„ç†èåˆæƒé‡ ---
    if fusion_weights is None:
        # ä½¿ç”¨ç­‰æƒé‡
        fusion_weights = [1.0 / len(model_names_list)] * len(model_names_list)
        print(f"Using equal weights: {fusion_weights}")
    else:
        # éªŒè¯æƒé‡
        if len(fusion_weights) != len(model_names_list):
            raise ValueError(f"fusion_weights length ({len(fusion_weights)}) must match model_names_list length ({len(model_names_list)})")
        
        # å½’ä¸€åŒ–æƒé‡
        total_weight = sum(fusion_weights)
        fusion_weights = [w / total_weight for w in fusion_weights]
        print(f"Using custom weights: {fusion_weights}")
    
    # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
    )
    base_model.eval()
    params_base = base_model.state_dict()
    
    fine_tuned_params_list = []
    for model_name in model_names_list:
        print(f"Loading fine-tuned model: {model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
        )
        model.eval()
        fine_tuned_params_list.append(model.state_dict())
        del model
    gc.collect()
    
    # --- 2. è®¡ç®—ä»»åŠ¡å‘é‡ ---
    print("Computing task vectors...")
    task_vectors = []
    param_names = list(params_base.keys())
    
    for i, ft_params in enumerate(fine_tuned_params_list):
        print(f"Computing task vector {i+1}/{len(fine_tuned_params_list)}")
        task_vector = {}
        
        for param_name in tqdm(param_names, desc=f"Computing TV {i+1}"):
            if param_name in ft_params:
                base_tensor = params_base[param_name]
                ft_tensor = ft_params[param_name]
                task_vector[param_name] = ft_tensor - base_tensor
        
        task_vectors.append(task_vector)
    
    # --- 3. æå–ç¨€ç–ç»„ä»¶ ---
    print(f"Extracting sparse components using {extraction_method} method...")
    sparse_components = []
    
    for i, task_vector in enumerate(task_vectors):
        print(f"Extracting sparse components for model {i+1}/{len(task_vectors)}")
        sparse_component = {}
        
        for param_name in tqdm(param_names, desc=f"Extracting SC {i+1}"):
            if param_name in task_vector:
                raw_vector = task_vector[param_name]
                
                if raw_vector.ndim > 0:  # è·³è¿‡æ ‡é‡å¼ é‡
                    # æ ¹æ®æå–æ–¹æ³•å¤„ç†
                    if extraction_method == "magnitude":
                        sparse_component[param_name] = extract_magnitude_components(
                            raw_vector, sparsity_threshold, component_ratio
                        )
                    elif extraction_method == "gradient":
                        sparse_component[param_name] = extract_gradient_components(
                            raw_vector, sparsity_threshold, component_ratio
                        )
                    elif extraction_method == "activation":
                        sparse_component[param_name] = extract_activation_components(
                            raw_vector, sparsity_threshold, component_ratio
                        )
                    else:
                        raise ValueError(f"Unsupported extraction method: {extraction_method}")
                else:
                    sparse_component[param_name] = raw_vector
        
        sparse_components.append(sparse_component)
    
    # --- 4. åˆå¹¶ç¨€ç–ç»„ä»¶ ---
    print("Merging sparse components with fusion weights...")
    merged_components = {}
    
    for param_name in tqdm(param_names, desc="Merging components"):
        if param_name in params_base:
            # æ”¶é›†æ‰€æœ‰ç¨€ç–ç»„ä»¶ä¸­è¯¥å‚æ•°çš„å€¼
            component_values = []
            for sc in sparse_components:
                if param_name in sc:
                    component_values.append(sc[param_name])
            
            if component_values:
                # ä½¿ç”¨èåˆæƒé‡è¿›è¡ŒåŠ æƒå¹³å‡åˆå¹¶
                weighted_sum = torch.zeros_like(component_values[0])
                for i, component in enumerate(component_values):
                    weighted_sum += fusion_weights[i] * component
                merged_component = weighted_sum
                merged_components[param_name] = merged_component
    
    # --- 5. æ„å»ºæœ€ç»ˆæ¨¡å‹ ---
    print("Building final merged model...")
    final_params = params_base.copy()
    
    for param_name in tqdm(param_names, desc="Building final model"):
        if param_name in merged_components:
            final_params[param_name] = params_base[param_name] + scaling_lambda * merged_components[param_name]
    
    # --- 6. åŠ è½½åˆå¹¶åçš„å‚æ•°å¹¶ä¿å­˜æ¨¡å‹ ---
    print("Loading merged parameters...")
    base_model.load_state_dict(final_params)
    
    # --- 7. ä¿å­˜æ¨¡å‹ ---
    save_model_and_tokenizer(base_model, base_model_name, output_path)
    
    # --- 8. æ¸…ç†å†…å­˜ ---
    del base_model, fine_tuned_params_list, task_vectors, sparse_components, merged_components, final_params
    gc.collect()
    torch.cuda.empty_cache()
    
def weighted_average_models(model_name_a, model_name_b, lambda_val, output_path):
    """Performs a weighted average: lambda * model_a + (1 - lambda) * model_b."""
    print("-" * 80)
    print(f"Starting weighted average with lambda = {lambda_val}")
    model_a = AutoModelForCausalLM.from_pretrained(model_name_a, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
    model_a.eval()
    model_b = AutoModelForCausalLM.from_pretrained(model_name_b, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
    model_b.eval()
    params_a = model_a.state_dict()
    params_b = model_b.state_dict()
    merged_params = {}
    for param_name in tqdm(params_a.keys(), desc=f"Averaging (lambda={lambda_val})"):
        if param_name in params_b:
            tensor_a = params_a[param_name]
            tensor_b = params_b[param_name].to(tensor_a.device, dtype=tensor_a.dtype)
            merged_params[param_name] = (lambda_val * tensor_a) + ((1 - lambda_val) * tensor_b)
        else:
            merged_params[param_name] = params_a[param_name]
    model_a.load_state_dict(merged_params)
    save_model_and_tokenizer(model_a, model_name_a, output_path)
    del model_a, model_b, params_a, params_b, merged_params
    gc.collect()
    torch.cuda.empty_cache()


def merge_and_amplify_top_k_diff_models(base_model_name, donor_model_name, percentage, output_path):
    """Merges and amplifies the top k% of differing parameters"""
    print("-" * 80)
    print(f"Starting SURGICAL MERGE WITH AMPLIFICATION for top {percentage}% parameters")
    model_a = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_a.eval()
    model_b = AutoModelForCausalLM.from_pretrained(donor_model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_b.eval()
    params_a = model_a.state_dict()
    params_b = model_b.state_dict()
    all_diffs_cpu = []
    total_params = 0
    for param_name in tqdm(params_a.keys(), desc="Calculating Diffs"):
        if param_name in params_b:
            tensor_a = params_a[param_name]
            total_params += tensor_a.numel()
            tensor_b = params_b[param_name].to(tensor_a.device, dtype=tensor_a.dtype)
            diff = torch.abs(tensor_b - tensor_a)
            all_diffs_cpu.append(diff.flatten().cpu())
    flat_all_diffs_cpu = torch.cat(all_diffs_cpu)
    num_to_merge = int(total_params * (percentage / 100.0))
    k_val_for_threshold = max(1, total_params - num_to_merge)
    threshold = torch.kthvalue(flat_all_diffs_cpu.to(torch.float32), k_val_for_threshold).values
    del flat_all_diffs_cpu, all_diffs_cpu
    gc.collect()
    alpha = 1.0 / (percentage / 100.0) if percentage > 0 else 1.0
    merged_params = model_a.state_dict()
    for param_name in tqdm(params_a.keys(), desc=f"Amplifying Top {percentage}%"):
        if param_name in params_b:
            tensor_a = params_a[param_name]
            tensor_b = params_b[param_name].to(tensor_a.device, dtype=tensor_a.dtype)
            diff_mask = torch.abs(tensor_b - tensor_a) >= threshold
            amplified_update = tensor_a + alpha * (tensor_b - tensor_a)
            merged_params[param_name] = torch.where(diff_mask, amplified_update, tensor_a)
    model_a.load_state_dict(merged_params)
    save_model_and_tokenizer(model_a, base_model_name, output_path)
    del model_a, model_b, params_a, params_b, merged_params
    gc.collect()
    torch.cuda.empty_cache()


def merge_top_k_diff_models(base_model_name, donor_model_name, percentage, output_path):
    """Merges the top k% of differing parameters from a donor model into a base model. è¿™ä¸ªæ˜¯surgical merge"""
    print("-" * 80)
    print(f"Starting surgical merge of top {percentage}% differing parameters")
    model_a = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
    model_a.eval()
    model_b = AutoModelForCausalLM.from_pretrained(donor_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
    model_b.eval()
    params_a = model_a.state_dict()
    params_b = model_b.state_dict()
    all_diffs_cpu = []
    total_params = 0
    for param_name in tqdm(params_a.keys(), desc="Calculating Diffs"):
        if param_name in params_b:
            tensor_a = params_a[param_name]
            total_params += tensor_a.numel()
            tensor_b = params_b[param_name].to(tensor_a.device, dtype=tensor_a.dtype)
            diff = torch.abs(tensor_b - tensor_a)
            all_diffs_cpu.append(diff.flatten().cpu())
    flat_all_diffs_cpu = torch.cat(all_diffs_cpu)
    num_to_merge = int(total_params * (percentage / 100.0))
    k_val_for_threshold = max(1, total_params - num_to_merge)
    threshold = torch.kthvalue(flat_all_diffs_cpu.to(torch.float32), k_val_for_threshold).values
    del flat_all_diffs_cpu, all_diffs_cpu
    gc.collect()
    merged_params = model_a.state_dict()
    for param_name in tqdm(params_a.keys(), desc=f"Merging Top {percentage}%"):
        if param_name in params_b:
            tensor_a = params_a[param_name]
            tensor_b = params_b[param_name].to(tensor_a.device, dtype=tensor_a.dtype)
            diff_mask = torch.abs(tensor_b - tensor_a) >= threshold
            merged_params[param_name] = torch.where(diff_mask, tensor_b, tensor_a)
    model_a.load_state_dict(merged_params)
    save_model_and_tokenizer(model_a, base_model_name, output_path)
    del model_a, model_b, params_a, params_b, merged_params
    gc.collect()
    torch.cuda.empty_cache()
    
def merge_top_k_avg_keep_base(base_model_name, instruct_model_name, percentage, output_path):
    """
    1. k%å·®å¼‚æœ€å¤§çš„å‚æ•°å–å¹³å‡ï¼Œå…¶ä»–çš„å‚æ•°ä¿ç•™æ¨ç†æ¨¡å‹çš„å‚æ•°ã€‚
    Averages the top k% most differing parameters, keeps the rest from the base (inference) model.
    """
    print("-" * 80)
    print(f"Starting merge: Top {percentage}% Avg, Keep Base")
    # Load models
    model_base = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
    model_base.eval()
    model_instruct = AutoModelForCausalLM.from_pretrained(instruct_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
    model_instruct.eval()
    
    params_base = model_base.state_dict()
    params_instruct = model_instruct.state_dict()
    
    # Calculate differences and find threshold for top k%
    all_diffs_cpu = []
    total_params = 0
    for param_name in tqdm(params_base.keys(), desc="Calculating Diffs"):
        if param_name in params_instruct:
            tensor_base = params_base[param_name]
            total_params += tensor_base.numel()
            tensor_instruct = params_instruct[param_name].to(tensor_base.device, dtype=tensor_base.dtype)
            diff = torch.abs(tensor_instruct - tensor_base)
            all_diffs_cpu.append(diff.flatten().cpu())
            
    flat_all_diffs_cpu = torch.cat(all_diffs_cpu)
    num_to_merge = int(total_params * (percentage / 100.0))
    k_val_for_threshold = min(num_to_merge, total_params)  # ç¬¬ k å°çš„å€¼
    threshold = torch.kthvalue(flat_all_diffs_cpu.to(torch.float32), k_val_for_threshold).values
    del flat_all_diffs_cpu, all_diffs_cpu
    gc.collect()
    
    # Apply merging logic
    merged_params = model_base.state_dict()
    for param_name in tqdm(params_base.keys(), desc=f"Merging Top {percentage}% Avg"):
        if param_name in params_instruct:
            tensor_base = params_base[param_name]
            tensor_instruct = params_instruct[param_name].to(tensor_base.device, dtype=tensor_base.dtype)
            
            diff_mask = torch.abs(tensor_instruct - tensor_base) <= threshold
            avg_tensor = (tensor_base + tensor_instruct) / 2.0
            
            # Where mask is True (top k%), use the average; otherwise, keep the base model's parameter.
            merged_params[param_name] = torch.where(diff_mask, avg_tensor, tensor_base)
            
    # Load merged state and save
    model_base.load_state_dict(merged_params)
    save_model_and_tokenizer(model_base, base_model_name, output_path)
    
    # Cleanup
    del model_base, model_instruct, params_base, params_instruct, merged_params
    gc.collect()
    torch.cuda.empty_cache()

def merge_top_k_avg_keep_base_min_diff(base_model_name, instruct_model_name, percentage, output_path):
    """
    å¯¹å·®å¼‚æœ€å°çš„ k% å‚æ•°å–å¹³å‡ï¼Œå…¶ä½™å‚æ•°ä¿ç•™ base æ¨¡å‹ã€‚
    Averages the top k% LEAST differing parameters, keeps the rest from the base model.
    """
    print("-" * 80)
    print(f"Starting merge: Top {percentage}% Min-Diff Avg, Keep Base")
    model_base = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_base.eval()
    model_instruct = AutoModelForCausalLM.from_pretrained(instruct_model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_instruct.eval()
    
    params_base = model_base.state_dict()
    params_instruct = model_instruct.state_dict()
    
    # Calculate absolute differences
    all_diffs_cpu = []
    total_params = 0
    for param_name in tqdm(params_base.keys(), desc="Calculating Diffs"):
        if param_name in params_instruct:
            tensor_base = params_base[param_name]
            total_params += tensor_base.numel()
            tensor_instruct = params_instruct[param_name].to(tensor_base.device, dtype=tensor_base.dtype)
            diff = torch.abs(tensor_instruct - tensor_base)
            all_diffs_cpu.append(diff.flatten().cpu())
            
    flat_all_diffs_cpu = torch.cat(all_diffs_cpu)
    num_to_merge = int(total_params * (percentage / 100.0))
    k_val_for_threshold = min(num_to_merge, total_params)  # ç¬¬ k å°çš„å€¼
    threshold = torch.kthvalue(flat_all_diffs_cpu.to(torch.float32), k_val_for_threshold).values
    del flat_all_diffs_cpu, all_diffs_cpu
    gc.collect()
    
    # Apply merging: avg for min-diff params, keep base for others
    merged_params = model_base.state_dict()
    for param_name in tqdm(params_base.keys(), desc=f"Merging Top {percentage}% Min-Diff Avg"):
        if param_name in params_instruct:
            tensor_base = params_base[param_name]
            tensor_instruct = params_instruct[param_name].to(tensor_base.device, dtype=tensor_base.dtype)
            diff_mask = torch.abs(tensor_instruct - tensor_base) <= threshold  # æœ€å°å·®å¼‚åŒºåŸŸ
            avg_tensor = (tensor_base + tensor_instruct) / 2.0
            merged_params[param_name] = torch.where(diff_mask, avg_tensor, tensor_base)
            
    model_base.load_state_dict(merged_params)
    save_model_and_tokenizer(model_base, base_model_name, output_path)
    
    del model_base, model_instruct, params_base, params_instruct, merged_params
    gc.collect()
    torch.cuda.empty_cache()

def merge_avg_override_top_k_base(base_model_name, instruct_model_name, percentage, output_path):
    """
    3. æ‰€æœ‰å‚æ•°éƒ½å–å¹³å‡ï¼Œä½†å·®å¼‚æœ€å¤§çš„k%å‚æ•°æ¢æˆæ¨ç†æ¨¡å‹çš„å‚æ•°ã€‚
    Averages all parameters, but then replaces the top k% most differing parameters with the base (inference) model's original parameters.
    """
    print("-" * 80)
    print(f"Starting merge: Average All, Override Top {percentage}% with Base")
    # Load models
    model_base = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
    model_base.eval()
    model_instruct = AutoModelForCausalLM.from_pretrained(instruct_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True)
    model_instruct.eval()

    params_base = model_base.state_dict()
    params_instruct = model_instruct.state_dict()

    # Calculate differences and find threshold for top k%
    all_diffs_cpu = []
    total_params = 0
    for param_name in tqdm(params_base.keys(), desc="Calculating Diffs"):
        if param_name in params_instruct:
            tensor_base = params_base[param_name]
            total_params += tensor_base.numel()
            tensor_instruct = params_instruct[param_name].to(tensor_base.device, dtype=tensor_base.dtype)
            diff = torch.abs(tensor_instruct - tensor_base)
            all_diffs_cpu.append(diff.flatten().cpu())

    flat_all_diffs_cpu = torch.cat(all_diffs_cpu)
    num_to_merge = int(total_params * (percentage / 100.0))
    k_val_for_threshold = max(1, total_params - num_to_merge)
    threshold = torch.kthvalue(flat_all_diffs_cpu.to(torch.float32), k_val_for_threshold).values
    del flat_all_diffs_cpu, all_diffs_cpu
    gc.collect()

    # Apply merging logic
    merged_params = model_base.state_dict()
    for param_name in tqdm(params_base.keys(), desc=f"Averaging and Overriding Top {percentage}%"):
        if param_name in params_instruct:
            tensor_base = params_base[param_name]
            tensor_instruct = params_instruct[param_name].to(tensor_base.device, dtype=tensor_base.dtype)
            
            diff_mask = torch.abs(tensor_instruct - tensor_base) >= threshold
            avg_tensor = (tensor_base + tensor_instruct) / 2.0
            
            # Where mask is True (top k%), override the average with the base model's parameter; otherwise, keep the average.
            merged_params[param_name] = torch.where(diff_mask, tensor_base, avg_tensor)

    # Load merged state and save
    model_base.load_state_dict(merged_params)
    save_model_and_tokenizer(model_base, base_model_name, output_path)

    # Cleanup
    del model_base, model_instruct, params_base, params_instruct, merged_params
    gc.collect()
    torch.cuda.empty_cache()

def merge_bottom_k_avg_keep_instruct(base_model_name, instruct_model_name, percentage, output_path):
    """
    k%å·®å¼‚æœ€minçš„å‚æ•°å–å¹³å‡ï¼Œå…¶ä»–çš„å‚æ•°ä¿ç•™Instructæ¨¡å‹çš„å‚æ•°ã€‚
    Averages the bottom k% most differing parameters, keeps the rest from the Instruct model.
    """
    print("-" * 80)
    print(f"Starting merge: Top {percentage}% Avg, Keep Instruct")
    # Load models
    model_base = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_base.eval()
    model_instruct = AutoModelForCausalLM.from_pretrained(instruct_model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_instruct.eval()

    params_base = model_base.state_dict()
    params_instruct = model_instruct.state_dict()

    # Calculate differences and find threshold for top k%
    all_diffs_cpu = []
    total_params = 0
    for param_name in tqdm(params_base.keys(), desc="Calculating Diffs"):
        if param_name in params_instruct:
            tensor_base = params_base[param_name]
            total_params += tensor_base.numel()
            tensor_instruct = params_instruct[param_name].to(tensor_base.device, dtype=tensor_base.dtype)
            diff = torch.abs(tensor_instruct - tensor_base)
            all_diffs_cpu.append(diff.flatten().cpu())

    flat_all_diffs_cpu = torch.cat(all_diffs_cpu)
    num_to_merge = int(total_params * (percentage / 100.0))
    k_val_for_threshold = min(num_to_merge, total_params)  # ç¬¬ k å°çš„å€¼
    threshold = torch.kthvalue(flat_all_diffs_cpu.to(torch.float32), k_val_for_threshold).values
    del flat_all_diffs_cpu, all_diffs_cpu
    gc.collect()
    
    # Apply merging logic
    merged_params = model_base.state_dict()
    for param_name in tqdm(params_base.keys(), desc=f"Merging Top {percentage}% Avg"):
        if param_name in params_instruct:
            tensor_base = params_base[param_name]
            tensor_instruct = params_instruct[param_name].to(tensor_base.device, dtype=tensor_base.dtype)
            
            diff_mask = torch.abs(tensor_instruct - tensor_base) <= threshold
            avg_tensor = (tensor_base + tensor_instruct) / 2.0
            
            # Where mask is True (top k%), use the average; otherwise, keep the instruct model's parameter.
            merged_params[param_name] = torch.where(diff_mask, avg_tensor, tensor_instruct)

    # Load merged state and save
    model_base.load_state_dict(merged_params)
    # We use the base model's tokenizer as is standard practice
    save_model_and_tokenizer(model_base, base_model_name, output_path)

    # Cleanup
    del model_base, model_instruct, params_base, params_instruct, merged_params
    gc.collect()
    torch.cuda.empty_cache()

def dare_merge_models_TA(base_model_name, model_a_name, model_b_name, drop_rate, scaling_lambda, output_path):
    """
    ä½¿ç”¨ DARE (Drop And REscale) å’Œä»»åŠ¡ç®—æœ¯ (Task Arithmetic) æ–¹æ³•åˆå¹¶ä¸¤ä¸ªæ¨¡å‹ã€‚
    å…¬å¼: Merged = Base + Î» * (DARE(Model_A - Base) + (1 - Î») * (DARE(Model_B - Base)

    Args:
        base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_a_name (str): ç¬¬ä¸€ä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_b_name (str): ç¬¬äºŒä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        drop_rate (float): DARE çš„ä¸¢å¼ƒç‡ p (0.0 åˆ° 1.0)ã€‚ä¾‹å¦‚ï¼Œ0.9 è¡¨ç¤ºéšæœºä¸¢å¼ƒ90%çš„å¢é‡å‚æ•°ã€‚
        scaling_lambda (float): ä»»åŠ¡ç®—æœ¯çš„ç¼©æ”¾ç³»æ•° Î»ã€‚
        output_path (str): åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚
    """
    print("-" * 80)
    print(f"Starting DARE merge with drop_rate={drop_rate} and scaling_lambda={scaling_lambda}")

    # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    base_model.eval()
    print("Loading model A...")
    model_a = AutoModelForCausalLM.from_pretrained(model_a_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_a.eval()
    print("Loading model B...")
    model_b = AutoModelForCausalLM.from_pretrained(model_b_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_b.eval()

    params_base = base_model.state_dict()
    params_a = model_a.state_dict()
    params_b = model_b.state_dict()
    
    merged_params = {}

    # --- 2. éå†å‚æ•°ï¼Œåº”ç”¨ DARE å’Œä»»åŠ¡ç®—æœ¯ ---
    for param_name in tqdm(params_base.keys(), desc="Applying DARE and Merging"):
        if param_name in params_a and param_name in params_b:
            base_tensor = params_base[param_name].to(torch.device("cuda"))
            tensor_a = params_a[param_name].to(base_tensor.device, dtype=base_tensor.dtype)
            tensor_b = params_b[param_name].to(base_tensor.device, dtype=base_tensor.dtype)

            # --- è®¡ç®—å¢é‡å‚æ•° (Deltas) ---
            delta_a = tensor_a - base_tensor
            delta_b = tensor_b - base_tensor

            # --- å¯¹æ¯ä¸ª Delta åº”ç”¨ DARE ---
            # DARE for Delta A
            mask_a = torch.rand_like(delta_a) > drop_rate
            dare_delta_a = (delta_a * mask_a) / (1 - drop_rate)
            
            # DARE for Delta B
            mask_b = torch.rand_like(delta_b) > drop_rate
            dare_delta_b = (delta_b * mask_b) / (1 - drop_rate)

            # --- ä½¿ç”¨ä»»åŠ¡ç®—æœ¯åˆå¹¶å¤„ç†åçš„ Deltas ---
            merged_delta = scaling_lambda * dare_delta_a + (1 - scaling_lambda) * dare_delta_b
            
            # --- å°†åˆå¹¶åçš„ Delta åº”ç”¨äºåŸºç¡€æ¨¡å‹ ---
            merged_params[param_name] = base_tensor + merged_delta
    #   else:
    #       # å¦‚æœæŸä¸ªå‚æ•°ä¸å­˜åœ¨äºæŸä¸ªæ¨¡å‹ä¸­ï¼Œåˆ™ç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„å‚æ•°
    #       merged_params[param_name] = params_base[param_name]

    # --- 3. åŠ è½½åˆå¹¶åçš„å‚æ•°å¹¶ä¿å­˜æ¨¡å‹ ---
    base_model.load_state_dict(merged_params)
    # Tokenizer é€šå¸¸ä½¿ç”¨åŸºç¡€æ¨¡å‹æˆ–å…¶ä¸­ä¸€ä¸ªå¾®è°ƒæ¨¡å‹çš„å³å¯
    save_model_and_tokenizer(base_model, base_model_name, output_path)

    # --- 4. æ¸…ç†å†…å­˜ ---
    del base_model, model_a, model_b, params_base, params_a, params_b, merged_params
    gc.collect()
    torch.cuda.empty_cache()


def dare_merge_models(base_model_name, model_a_name, model_b_name, drop_rate, scaling_lambda, output_path):
    """
    ä½¿ç”¨ DARE (Drop And REscale) å’Œä»»åŠ¡ç®—æœ¯ (Task Arithmetic) æ–¹æ³•åˆå¹¶ä¸¤ä¸ªæ¨¡å‹ã€‚
    å…¬å¼: Merged = Base + Î» * (DARE(Model_A - Base) + Î» * (DARE(Model_B - Base)

    Args:
        base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_a_name (str): ç¬¬ä¸€ä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_b_name (str): ç¬¬äºŒä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        drop_rate (float): DARE çš„ä¸¢å¼ƒç‡ p (0.0 åˆ° 1.0)ã€‚ä¾‹å¦‚ï¼Œ0.9 è¡¨ç¤ºéšæœºä¸¢å¼ƒ90%çš„å¢é‡å‚æ•°ã€‚
        scaling_lambda (float): ä»»åŠ¡ç®—æœ¯çš„ç¼©æ”¾ç³»æ•° Î»ã€‚
        output_path (str): åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚
    """
    print("-" * 80)
    print(f"Starting DARE merge with drop_rate={drop_rate} and scaling_lambda={scaling_lambda}")

    # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    base_model.eval()
    print("Loading model A...")
    model_a = AutoModelForCausalLM.from_pretrained(model_a_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_a.eval()
    print("Loading model B...")
    model_b = AutoModelForCausalLM.from_pretrained(model_b_name, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    model_b.eval()

    params_base = base_model.state_dict()
    params_a = model_a.state_dict()
    params_b = model_b.state_dict()
    
    merged_params = {}

    # --- 2. éå†å‚æ•°ï¼Œåº”ç”¨ DARE å’Œä»»åŠ¡ç®—æœ¯ ---
    for param_name in tqdm(params_base.keys(), desc="Applying DARE and Merging"):
        if param_name in params_a and param_name in params_b:
            base_tensor = params_base[param_name].to(torch.device("cuda"))
            tensor_a = params_a[param_name].to(base_tensor.device, dtype=base_tensor.dtype)
            tensor_b = params_b[param_name].to(base_tensor.device, dtype=base_tensor.dtype)

            # --- è®¡ç®—å¢é‡å‚æ•° (Deltas) ---
            delta_a = tensor_a - base_tensor
            delta_b = tensor_b - base_tensor

            # --- å¯¹æ¯ä¸ª Delta åº”ç”¨ DARE ---
            # DARE for Delta A
            mask_a = torch.rand_like(delta_a) > drop_rate
            dare_delta_a = (delta_a * mask_a) / (1 - drop_rate)
            
            # DARE for Delta B
            mask_b = torch.rand_like(delta_b) > drop_rate
            dare_delta_b = (delta_b * mask_b) / (1 - drop_rate)

            # --- ä½¿ç”¨ä»»åŠ¡ç®—æœ¯åˆå¹¶å¤„ç†åçš„ Deltas ---
            merged_delta = scaling_lambda * dare_delta_a + scaling_lambda * dare_delta_b
            
            # --- å°†åˆå¹¶åçš„ Delta åº”ç”¨äºåŸºç¡€æ¨¡å‹ ---
            merged_params[param_name] = base_tensor + merged_delta
    #   else:
    #       # å¦‚æœæŸä¸ªå‚æ•°ä¸å­˜åœ¨äºæŸä¸ªæ¨¡å‹ä¸­ï¼Œåˆ™ç›´æ¥ä½¿ç”¨åŸºç¡€æ¨¡å‹çš„å‚æ•°
    #       merged_params[param_name] = params_base[param_name]

    # --- 3. åŠ è½½åˆå¹¶åçš„å‚æ•°å¹¶ä¿å­˜æ¨¡å‹ ---
    base_model.load_state_dict(merged_params)
    # Tokenizer é€šå¸¸ä½¿ç”¨åŸºç¡€æ¨¡å‹æˆ–å…¶ä¸­ä¸€ä¸ªå¾®è°ƒæ¨¡å‹çš„å³å¯
    save_model_and_tokenizer(base_model, base_model_name, output_path)

    # --- 4. æ¸…ç†å†…å­˜ ---
    del base_model, model_a, model_b, params_base, params_a, params_b, merged_params
    gc.collect()
    torch.cuda.empty_cache()

def ties_merge_models(base_model_name, model_names_list, top_k_percentage, scaling_lambda, output_path):
    """
    ä½¿ç”¨å†…å­˜ä¼˜åŒ–çš„ TIES-Merging (Trim, Elect Sign & Merge) æ–¹æ³•åˆå¹¶å¤šä¸ªæ¨¡å‹ã€‚
    æ­¤ç‰ˆæœ¬é€šè¿‡é€ä¸ªå‚æ•°å¤„ç†æ¥é¿å…åˆ›å»ºå·¨å¤§çš„ä¸­é—´å¼ é‡ï¼Œä»è€Œè§£å†³å†…å­˜æº¢å‡ºï¼ˆOOMï¼‰é—®é¢˜ã€‚
    æ³¨æ„ï¼šæ­¤å®ç°æ˜¯ TIES è®ºæ–‡ç®—æ³•çš„ä¸€ç§è¿‘ä¼¼ï¼Œå®ƒåœ¨æ¯ä¸ªå‚æ•°å¼ é‡ä¸Šè¿›è¡Œå±€éƒ¨ä¿®å‰ªï¼ˆlocal trimmingï¼‰ï¼Œ
    è€Œéå…¨å±€ä¿®å‰ªï¼ˆglobal trimmingï¼‰ï¼Œä½†è¿™å¯¹äºè§£å†³å†…å­˜ç“¶é¢ˆè‡³å…³é‡è¦ã€‚

    Args:
        base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_names_list (list[str]): éœ€è¦åˆå¹¶çš„å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„åˆ—è¡¨ã€‚
        top_k_percentage (float): Trim æ­¥éª¤ä¸­ä¿ç•™çš„å‚æ•°ç™¾åˆ†æ¯” (0-100)ã€‚å®ƒå°†åº”ç”¨äºæ¯ä¸ªå‚æ•°å¼ é‡ã€‚
        scaling_lambda (float): æœ€ç»ˆåˆå¹¶ä»»åŠ¡å‘é‡çš„ç¼©æ”¾ç³»æ•° Î»ã€‚
        output_path (str): åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚
    """
    print("-" * 80)
    print(f"Starting Memory-Optimized TIES-Merging with top_k={top_k_percentage}% and scaling_lambda={scaling_lambda}")

    # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
    )
    base_model.eval()
    params_base = base_model.state_dict()

    fine_tuned_params_list = []
    for model_name in model_names_list:
        print(f"Loading fine-tuned model: {model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
        )
        model.eval()
        fine_tuned_params_list.append(model.state_dict())
        del model
    gc.collect()

    final_merged_task_vector = {}
    param_names = list(params_base.keys())
    
    # --- é€ä¸ªå‚æ•°è¿›è¡Œ TIES åˆå¹¶ ---
    for param_name in tqdm(param_names, desc="TIES-Merging (per-parameter)"):
        base_tensor = params_base[param_name]
        
        # --- è®¡ç®—å½“å‰å‚æ•°çš„ä»»åŠ¡å‘é‡ ---
        tvs_for_param = [ft_params[param_name] - base_tensor for ft_params in fine_tuned_params_list]

        # --- Step 1: Trim (Local Approximation) ---
        trimmed_tvs_for_param = []
        for tensor in tvs_for_param:
            if tensor.ndim == 0:  # Skip scalar tensors
                trimmed_tvs_for_param.append(tensor)
                continue

            num_params_to_trim = int(tensor.numel() * (1 - top_k_percentage / 100.0))
            if num_params_to_trim > 0:
                # æ‰¾åˆ°å±€éƒ¨é˜ˆå€¼
                threshold = torch.kthvalue(torch.abs(tensor).flatten(), num_params_to_trim).values
                mask = torch.abs(tensor) >= threshold
                trimmed_tvs_for_param.append(tensor * mask)
            else:
                trimmed_tvs_for_param.append(tensor)
        
        # --- Step 2: Elect Sign ---
        sum_of_trimmed_tvs = torch.sum(torch.stack(trimmed_tvs_for_param), dim=0)
        elected_sign_tensor = torch.sign(sum_of_trimmed_tvs)

        # --- Step 3: Disjoint Merge ---
        final_sum = torch.zeros_like(base_tensor)
        final_counts = torch.zeros_like(base_tensor)

        for trimmed_tensor in trimmed_tvs_for_param:
            agreement_mask = (torch.sign(trimmed_tensor) == elected_sign_tensor).float()
            final_sum += trimmed_tensor * agreement_mask
            final_counts += agreement_mask

        # é¿å…é™¤ä»¥é›¶
        merged_tensor = torch.where(
            final_counts > 0,
            final_sum / final_counts,
            0.0
        )
        final_merged_task_vector[param_name] = merged_tensor

    # æ¸…ç†å†…å­˜
    del fine_tuned_params_list, tvs_for_param, trimmed_tvs_for_param
    gc.collect()

    # --- å°†åˆå¹¶åçš„ä»»åŠ¡å‘é‡åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹ ---
    print("Applying merged task vector to the base model...")
    merged_params = base_model.state_dict()
    for param_name, tensor in final_merged_task_vector.items():
        merged_params[param_name] += (scaling_lambda * tensor).to(merged_params[param_name].device)

    base_model.load_state_dict(merged_params)
    
    # --- ä¿å­˜æ¨¡å‹ ---
    save_model_and_tokenizer(base_model, base_model_name, output_path)

    del base_model
    gc.collect()
    torch.cuda.empty_cache()


# def ties_merge_models_TA(base_model_name, model_names_list, top_k_percentage, scaling_lambda, output_path):
#     """
#     ã€æ··åˆç‰ˆæœ¬ã€‘
#     - å½“ model_names_list é•¿åº¦ä¸º 2 æ—¶ï¼šæ‰§è¡Œå‡¸ç»„åˆï¼ˆConvex Combinationï¼‰
#         Final = Î» * Model1 + (1-Î») * Model2
#     - å½“é•¿åº¦ > 2 æ—¶ï¼šæ‰§è¡Œéæ ‡å‡† TIESï¼ˆå…ˆä¹˜ Î»ï¼Œå†æŠ•ç¥¨ï¼‰

#     Args:
#         base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
#         model_names_list (list[str]): éœ€è¦åˆå¹¶çš„å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„åˆ—è¡¨ã€‚
#         top_k_percentage (float): Trim æ­¥éª¤ä¸­ä¿ç•™çš„å‚æ•°ç™¾åˆ†æ¯” (0-100)ã€‚å…¨å±€åº”ç”¨ã€‚
#         scaling_lambda (float): ç¼©æ”¾ç³»æ•° Î»ï¼ˆåœ¨å‡¸ç»„åˆä¸­è¡¨ç¤º Model1 çš„æƒé‡ï¼‰ã€‚
#         output_path (str): åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚
#     """
#     print("-" * 80)
#     n_models = len(model_names_list)
#     if n_models == 2:
#         print(f"Starting CONVEX COMBINATION for 2 models with weight_Î»={scaling_lambda}")
#     else:
#         print(f"Starting TIES-Merging (NON-STANDARD: Î» FIRST) with GLOBAL top_k={top_k_percentage}% and scaling_lambda={scaling_lambda}")

#     # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
#     print("Loading base model...")
#     base_model = AutoModelForCausalLM.from_pretrained(
#         base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
#     )
#     base_model.eval()
#     params_base = base_model.state_dict()

#     fine_tuned_params_list = []
#     for model_name in model_names_list:
#         print(f"Loading fine-tuned model: {model_name}")
#         model = AutoModelForCausalLM.from_pretrained(
#             model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
#         )
#         model.eval()
#         fine_tuned_params_list.append(model.state_dict())
#         del model
#     gc.collect()

#     # --- 2. å¦‚æœæ˜¯ 2 ä¸ªæ¨¡å‹ï¼Œç›´æ¥å‡¸ç»„åˆ ---
# # --- 2. å¦‚æœæ˜¯ 2 ä¸ªæ¨¡å‹ï¼Œæ‰§è¡Œå¸¦ Top-K ä¿®å‰ªçš„å‡¸ç»„åˆ ---
#     if n_models == 2:
#         print(f"Computing convex combination with Top-{top_k_percentage}% pruning...")
        
#         # --- 2.1 è®¡ç®—å…¨å±€é˜ˆå€¼ ---
#         print("Computing global threshold for Top-K pruning...")
#         all_tvs_abs_cpu = []
#         total_params = 0
#         param_names = list(params_base.keys())
        
#         for param_name in tqdm(param_names, desc="Collecting magnitudes for threshold", leave=False):
#             base_tensor = params_base[param_name]
#             tv1 = fine_tuned_params_list[0][param_name] - base_tensor
#             total_params += tv1.numel()
#             all_tvs_abs_cpu.append(torch.abs(tv1).flatten().cpu())
        
#         flat_all_tvs_abs_cpu = torch.cat(all_tvs_abs_cpu)
#         num_to_keep = int(total_params * (top_k_percentage / 100.0))
#         k_val_for_threshold = max(1, total_params - num_to_keep)
#         global_threshold = torch.kthvalue(flat_all_tvs_abs_cpu.to(torch.float32), k_val_for_threshold).values
        
#         del flat_all_tvs_abs_cpu, all_tvs_abs_cpu
#         gc.collect()
        
#         # --- 2.2 æ‰§è¡Œå‡¸ç»„åˆ ---
#         print("Applying convex combination with pruning...")
#         final_merged_task_vector = {}
        
#         for param_name in tqdm(param_names, desc="Convex Combination with Pruning"):
#             base_tensor = params_base[param_name]
#             tv1 = fine_tuned_params_list[0][param_name] - base_tensor
#             tv2 = fine_tuned_params_list[1][param_name] - base_tensor
            
#             # å¯¹ä¸¤ä¸ªä»»åŠ¡å‘é‡åˆ†åˆ«åº”ç”¨ Top-K ä¿®å‰ª
#             mask1 = torch.abs(tv1) >= global_threshold
#             mask2 = torch.abs(tv2) >= global_threshold
#             tv1_pruned = tv1 * mask1
#             tv2_pruned = tv2 * mask2
            
#             # å‡¸ç»„åˆï¼šÎ» * TV1 + (1-Î») * TV2
#             merged_tv = scaling_lambda * tv1_pruned + (1.0 - scaling_lambda) * tv2_pruned
#             final_merged_task_vector[param_name] = merged_tv
    
#         del fine_tuned_params_list
#         gc.collect()

#     # --- 3. å¦‚æœ >2 ä¸ªæ¨¡å‹ï¼Œæ‰§è¡ŒåŸâ€œå…ˆä¹˜ Î» å†æŠ•ç¥¨â€é€»è¾‘ ---
#     else:
#         # --- 3.1 å…¨å±€è®¡ç®—ä»»åŠ¡å‘é‡ç»å¯¹å€¼ï¼Œç”¨äº Global Trim é˜ˆå€¼è®¡ç®— ---
#         print("Computing global task vector magnitudes for global top-k threshold...")
#         all_tvs_abs_cpu = []
#         total_params = 0

#         param_names = list(params_base.keys())
#         for param_name in tqdm(param_names, desc="Collecting TV magnitudes"):
#             base_tensor = params_base[param_name]
#             if len(fine_tuned_params_list) == 0:
#                 continue
#             tv_tensor = fine_tuned_params_list[0][param_name] - base_tensor
#             total_params += tv_tensor.numel()
#             all_tvs_abs_cpu.append(torch.abs(tv_tensor).flatten().cpu())

#         flat_all_tvs_abs_cpu = torch.cat(all_tvs_abs_cpu)
#         num_to_keep = int(total_params * (top_k_percentage / 100.0))
#         k_val_for_threshold = max(1, total_params - num_to_keep)
#         global_threshold = torch.kthvalue(flat_all_tvs_abs_cpu.to(torch.float32), k_val_for_threshold).values

#         del flat_all_tvs_abs_cpu, all_tvs_abs_cpu
#         gc.collect()

#         # --- 3.2 é€ä¸ªå‚æ•°è¿›è¡Œ TIES åˆå¹¶ ---
#         final_merged_task_vector = {}
#         for param_name in tqdm(param_names, desc="TIES-Merging with Global Trim (Î» FIRST)"):
#             base_tensor = params_base[param_name]
#             tvs_for_param = [ft_params[param_name] - base_tensor for ft_params in fine_tuned_params_list]

#             # Trim
#             trimmed_tvs_for_param = []
#             for tensor in tvs_for_param:
#                 if tensor.ndim == 0:
#                     trimmed_tvs_for_param.append(tensor)
#                     continue
#                 mask = torch.abs(tensor) >= global_threshold
#                 trimmed_tvs_for_param.append(tensor * mask)
            
#             # å…ˆä¹˜ lambda
#             scaled_tvs_for_param = [tv * scaling_lambda for tv in trimmed_tvs_for_param]

#             # Elect Sign (on SCALED vectors)
#             sum_of_scaled_tvs = torch.sum(torch.stack(scaled_tvs_for_param), dim=0)
#             elected_sign_tensor = torch.sign(sum_of_scaled_tvs)

#             # Disjoint Merge
#             final_sum = torch.zeros_like(base_tensor)
#             final_counts = torch.zeros_like(base_tensor)
#             for scaled_tensor in scaled_tvs_for_param:
#                 agreement_mask = (torch.sign(scaled_tensor) == elected_sign_tensor).float()
#                 final_sum += scaled_tensor * agreement_mask
#                 final_counts += agreement_mask

#             merged_tensor = torch.where(final_counts > 0, final_sum / final_counts, 0.0)
#             final_merged_task_vector[param_name] = merged_tensor

#         del fine_tuned_params_list, tvs_for_param, trimmed_tvs_for_param, scaled_tvs_for_param
#         gc.collect()

#     # --- 4. å°†åˆå¹¶åçš„ä»»åŠ¡å‘é‡åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹ ---
#     print("Applying merged task vector to the base model...")
#     merged_params = base_model.state_dict()
#     for param_name, tensor in final_merged_task_vector.items():
#         merged_params[param_name] += tensor.to(merged_params[param_name].device)

#     base_model.load_state_dict(merged_params)
#     save_model_and_tokenizer(base_model, base_model_name, output_path)

#     # --- 5. æ¸…ç†å†…å­˜ ---
#     del base_model, final_merged_task_vector, merged_params
#     gc.collect()
#     torch.cuda.empty_cache()

#     if n_models == 2:
#         print("âœ… Convex Combination completed successfully!")
#     else:
#         print("âš ï¸  WARNING: This is a NON-STANDARD version (Î» applied BEFORE sign voting). Use for experimental purposes only.")

def twin_merge_models(base_model_name, model_names_list, mask_rate, scaling_lambda, output_path, router_model_path=None):
    """
    ä½¿ç”¨ Twin-Merging æ–¹æ³•åˆå¹¶å¤šä¸ªæ¨¡å‹ã€‚
    Twin-Merging åŒ…å«ä¸¤ä¸ªé˜¶æ®µï¼š
    1. æ¨¡å—åŒ–çŸ¥è¯†åˆ†è§£ï¼šå°†ä¸“å®¶æ¨¡å‹çŸ¥è¯†åˆ†è§£ä¸ºå…±äº«å’Œä¸“å±ç»„ä»¶
    2. åŠ¨æ€çŸ¥è¯†èåˆï¼šæ ¹æ®è¾“å…¥åŠ¨æ€èåˆå…±äº«å’Œä»»åŠ¡ç‰¹å®šçŸ¥è¯†
    
    Args:
        base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_names_list (list[str]): éœ€è¦åˆå¹¶çš„å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„åˆ—è¡¨ã€‚
        mask_rate (float): Twin Vector çš„ç¨€ç–åŒ–ç‡ (0.0 åˆ° 1.0)ã€‚ä¾‹å¦‚ï¼Œ0.8 è¡¨ç¤ºä¿ç•™20%çš„å‚æ•°ã€‚
        scaling_lambda (float): ä»»åŠ¡ç®—æœ¯çš„ç¼©æ”¾ç³»æ•° Î»ã€‚
        output_path (str): åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚
        router_model_path (str, optional): è·¯ç”±å™¨æ¨¡å‹çš„è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨ç®€å•å¹³å‡ã€‚
    """
    print("-" * 80)
    print(f"Starting Twin-Merging with mask_rate={mask_rate} and scaling_lambda={scaling_lambda}")
    
    # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
    )
    base_model.eval()
    params_base = base_model.state_dict()
    
    fine_tuned_params_list = []
    for model_name in model_names_list:
        print(f"Loading fine-tuned model: {model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
        )
        model.eval()
        fine_tuned_params_list.append(model.state_dict())
        del model
    gc.collect()
    
    # --- 2. ç¬¬ä¸€é˜¶æ®µï¼šè®¡ç®—å…±äº«æ¨¡å‹ï¼ˆä½¿ç”¨ç®€å•å¹³å‡ï¼Œä¸ä¹˜ scaling_lambdaï¼‰ ---
    print("Computing shared model using Simple Average...")
    shared_params = {}
    param_names = list(params_base.keys())
    
    for param_name in tqdm(param_names, desc="Computing Shared Model"):
        base_tensor = params_base[param_name]
        
        # è®¡ç®—ä»»åŠ¡å‘é‡
        task_vectors = []
        for ft_params in fine_tuned_params_list:
            if param_name in ft_params:
                task_vector = ft_params[param_name] - base_tensor
                task_vectors.append(task_vector)
        
        if task_vectors:
            # ç®€å•å¹³å‡ï¼ˆä¸ä¹˜ scaling_lambdaï¼Œç•™ç»™ç¬¬ä¸‰é˜¶æ®µæ§åˆ¶ï¼‰
            merged_task_vector = sum(task_vectors) / len(task_vectors)
            shared_params[param_name] = base_tensor + merged_task_vector  # ğŸ‘ˆ ä¸ä¹˜ lambda
        else:
            shared_params[param_name] = base_tensor
    
    # --- 3. ç¬¬äºŒé˜¶æ®µï¼šæå–Twin Vectors ---
    print("Extracting Twin Vectors...")
    twin_vectors = []
    
    for i, ft_params in enumerate(fine_tuned_params_list):
        print(f"Processing Twin Vector {i+1}/{len(fine_tuned_params_list)}")
        twin_vector = {}
        
        for param_name in tqdm(param_names, desc=f"Extracting TV {i+1}"):
            if param_name in ft_params and param_name in shared_params:
                base_tensor = params_base[param_name]
                ft_tensor = ft_params[param_name]
                shared_tensor = shared_params[param_name]
                
                # è®¡ç®—Twin Vector: Î¸^t - Î¸*
                raw_twin_vector = ft_tensor - shared_tensor
                
                # åº”ç”¨ç¨€ç–åŒ–ï¼ˆmagnitude-basedï¼‰
                if raw_twin_vector.ndim > 0:  # è·³è¿‡æ ‡é‡å¼ é‡
                    # è®¡ç®—è¦ä¿ç•™çš„å‚æ•°æ•°é‡
                    num_params = raw_twin_vector.numel()
                    num_to_keep = int(num_params * (1 - mask_rate))
                    
                    if num_to_keep > 0:
                        # æ‰¾åˆ°é˜ˆå€¼
                        abs_values = torch.abs(raw_twin_vector).flatten()
                        threshold = torch.kthvalue(abs_values, num_params - num_to_keep + 1).values
                        
                        # åˆ›å»ºæ©ç 
                        mask = torch.abs(raw_twin_vector) >= threshold
                        twin_vector[param_name] = raw_twin_vector * mask
                    else:
                        twin_vector[param_name] = torch.zeros_like(raw_twin_vector)
                else:
                    twin_vector[param_name] = raw_twin_vector
        
        twin_vectors.append(twin_vector)
    # --- 4. ç¬¬ä¸‰é˜¶æ®µï¼šåŠ¨æ€èåˆï¼ˆæ”¯æŒåŠ¨æ€æ¯”ä¾‹ï¼‰ ---
    print("Computing final merged model...")
    final_params = shared_params.copy()
    
    n_models = len(model_names_list)
    
    if router_model_path is None:
        print("Using dynamic weighting for Twin Vector combination...")
        for param_name in tqdm(param_names, desc="Combining Twin Vectors"):
            if param_name not in shared_params:
                continue

            tv_values = []
            for tv in twin_vectors:
                if param_name in tv:
                    tv_values.append(tv[param_name])
            
            if not tv_values:
                continue

            # æ ¸å¿ƒä¿®æ”¹ï¼šæ ¹æ®æ¨¡å‹æ•°é‡é€‰æ‹©èåˆç­–ç•¥
            if n_models == 2 and len(tv_values) == 2:
                # åŠ¨æ€æ¯”ä¾‹ï¼šÎ» * TV1 + (1-Î») * TV2
                combined_tv = scaling_lambda * tv_values[0] + (1.0 - scaling_lambda) * tv_values[1]
                print(f"  â†’ Using convex combination for {param_name} with Î»={scaling_lambda}")
            else:
                # ç®€å•å¹³å‡ï¼ˆæˆ–æœªæ¥å¯æ‰©å±•ä¸ºè‡ªå®šä¹‰æƒé‡ï¼‰
                combined_tv = sum(tv_values) / len(tv_values)

            final_params[param_name] = shared_params[param_name] + combined_tv
    else:
        print(f"Router-based combination not implemented yet. Using dynamic weighting...")
        # åŒä¸Šé€»è¾‘
        for param_name in tqdm(param_names, desc="Combining Twin Vectors"):
            if param_name not in shared_params:
                continue

            tv_values = []
            for tv in twin_vectors:
                if param_name in tv:
                    tv_values.append(tv[param_name])
            
            if not tv_values:
                continue

            if n_models == 2 and len(tv_values) == 2:
                combined_tv = scaling_lambda * tv_values[0] + (1.0 - scaling_lambda) * tv_values[1]
            else:
                combined_tv = sum(tv_values) / len(tv_values)

            final_params[param_name] = shared_params[param_name] + combined_tv
    # --- 5. åŠ è½½åˆå¹¶åçš„å‚æ•°å¹¶ä¿å­˜æ¨¡å‹ ---
    print("Loading merged parameters...")
    base_model.load_state_dict(final_params)
    
    # --- 6. ä¿å­˜æ¨¡å‹ ---
    save_model_and_tokenizer(base_model, base_model_name, output_path)
    
    # --- 7. æ¸…ç†å†…å­˜ ---
    del base_model, fine_tuned_params_list, shared_params, twin_vectors, final_params
    gc.collect()
    torch.cuda.empty_cache()
    
    print(f"Twin-Merging completed successfully! Model saved to: {output_path}")


def singular_value_thresholding(matrix: torch.Tensor, mu: float, device: str) -> torch.Tensor:
    """
    å®ç°è®ºæ–‡ä¸­çš„ SVT(Î´; Âµ) ç®—å­ï¼š
    1. å¯¹çŸ©é˜µåš SVD: Î´ = U Î£ V^T
    2. å¯¹å¥‡å¼‚å€¼åšè½¯é˜ˆå€¼: Î£âº = diag(max(Ïƒáµ¢ - Âµ, 0))
    3. é‡æ„çŸ©é˜µ: U Î£âº V^T
    """
    if matrix.numel() == 0:
        return matrix

    original_shape = matrix.shape
    if matrix.ndim != 2:
        m = matrix.size(0)
        matrix_2d = matrix.view(m, -1)
    else:
        matrix_2d = matrix

    matrix_2d = matrix_2d.to(device, dtype=torch.float32)

    try:
        U, S, Vt = torch.svd(matrix_2d)
        S_thresh = torch.clamp(S - mu, min=0.0)
        matrix_2d_lore = torch.mm(torch.mm(U, torch.diag(S_thresh)), Vt.T)
        result = matrix_2d_lore.view(original_shape).cpu()
    except Exception as e:
        print(f"[WARNING] SVD failed for shape {matrix.shape}, using zero matrix. Error: {e}")
        result = torch.zeros_like(matrix)

    return result

def lore_merge_models(
    base_model_name: str,
    model_names_list: List[str],
    task_weights: Optional[List[float]] = None,  # ğŸ‘ˆ æ–°å¢ï¼šåŠ¨æ€èåˆæƒé‡
    max_iter: int = 20,
    output_path: str = "./lore_merged_model",
    mu: float = 0.01,
    scaling_lambda: float = 1.0,

):
    """
    ã€æ”¯æŒåŠ¨æ€æƒé‡çš„ LoRE-Mergingã€‘
    åœ¨æœ€ç»ˆåˆå¹¶é˜¶æ®µï¼Œä½¿ç”¨ task_weights åŠ æƒæ±‚å’Œä»»åŠ¡å‘é‡ Î´áµ¢ã€‚

    Args:
        base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ã€‚
        model_names_list (list[str]): å¾®è°ƒæ¨¡å‹è·¯å¾„åˆ—è¡¨ã€‚
        task_weights (list[float], optional): æ¯ä¸ªæ¨¡å‹çš„èåˆæƒé‡ã€‚è‹¥ä¸º Noneï¼Œåˆ™å¹³å‡ã€‚
        mu (float): æ ¸èŒƒæ•°ç³»æ•° Âµï¼ˆé»˜è®¤ 0.01ï¼‰ã€‚
        max_iter (int): åæ ‡ä¸‹é™è¿­ä»£æ¬¡æ•°ï¼ˆé»˜è®¤ 20ï¼‰ã€‚
        scaling_lambda (float): æœ€ç»ˆç¼©æ”¾ç³»æ•° Î»ï¼ˆé»˜è®¤ 1.0ï¼‰ã€‚
        output_path (str): åˆå¹¶åæ¨¡å‹ä¿å­˜è·¯å¾„ã€‚
    """
    print("-" * 80)
    n_models = len(model_names_list)
    print(f"Starting LORE-MERGING (Dynamic Weighted) | Âµ={mu}, max_iter={max_iter}, Î»={scaling_lambda}")

    # ğŸ‘‡ æ–°å¢ï¼šå¤„ç†åŠ¨æ€æƒé‡
    if task_weights is None:
        task_weights = [1.0 / n_models] * n_models
        print("â†’ Using equal weights for all models")
    else:
        assert len(task_weights) == n_models, "task_weights é•¿åº¦å¿…é¡»ç­‰äºæ¨¡å‹æ•°é‡"
        total_weight = sum(task_weights)
        task_weights = [w / total_weight for w in task_weights]  # å½’ä¸€åŒ–
        print(f"â†’ Using custom weights: {task_weights}")

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
    print("Loading models...")
    model_params_list = []
    for model_name in model_names_list:
        print(f"Loading fine-tuned model: {model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.float32, device_map="cpu", trust_remote_code=True
        )
        model.eval()
        model_params_list.append(model.state_dict())
        del model
    gc.collect()

    # --- 2. åˆå§‹åŒ–ï¼šÎ´áµ¢ = 0, Î¸â‚€ = mean(Î¸áµ¢) ---
    print("Initializing approximate base model Î¸â‚€ and task vectors Î´áµ¢...")
    param_names = list(model_params_list[0].keys())

    deltas = [{} for _ in range(n_models)]
    for i in range(n_models):
        for param_name in param_names:
            deltas[i][param_name] = torch.zeros_like(model_params_list[i][param_name], dtype=torch.float32)

    theta_0 = {}
    for param_name in param_names:
        stacked = torch.stack([model_params_list[i][param_name] for i in range(n_models)])
        theta_0[param_name] = torch.mean(stacked, dim=0)
        del stacked
    gc.collect()

    # --- 3. åæ ‡ä¸‹é™ä¼˜åŒ–ï¼šÎ¸â‚€ å’Œ Î´áµ¢ è¿­ä»£æ›´æ–° ---
    print(f"Running coordinate descent for {max_iter} iterations...")
    for iteration in range(max_iter):
        # Step 1: æ›´æ–° Î¸â‚€ = 1/n Î£ (Î¸áµ¢ - Î´áµ¢)
        print(f"  â†’ Iteration {iteration + 1}/{max_iter}: Updating Î¸â‚€...")
        for param_name in tqdm(param_names, desc="Updating Î¸â‚€", leave=False):
            stacked_diff = torch.stack([
                model_params_list[i][param_name] - deltas[i][param_name]
                for i in range(n_models)
            ])
            theta_0[param_name] = torch.mean(stacked_diff, dim=0)
            del stacked_diff

        # Step 2: æ›´æ–° Î´áµ¢ = SVT(Î¸áµ¢ - Î¸â‚€; Âµ)
        print(f"  â†’ Iteration {iteration + 1}/{max_iter}: Updating Î´áµ¢ with SVT...")
        for i in range(n_models):
            for param_name in tqdm(param_names, desc=f"Updating Î´_{i}", leave=False):
                residual = model_params_list[i][param_name] - theta_0[param_name]
                deltas[i][param_name] = singular_value_thresholding(residual, mu, device)

        gc.collect()

    # --- 4. è®¡ç®—æœ€ç»ˆåˆå¹¶æ¨¡å‹ï¼ˆæ”¯æŒåŠ¨æ€æƒé‡ï¼ï¼‰---
    print("Computing final merged model with dynamic weights...")

    # ğŸ‘‡ æ ¸å¿ƒæ”¹åŠ¨ï¼šåŠ æƒæ±‚å’Œ Ï„ = Î£ (wáµ¢ * Î´áµ¢)
    tau = {}
    for param_name in tqdm(param_names, desc="Weighted sum of task vectors"):
        weighted_sum = None
        for i in range(n_models):
            delta_weighted = deltas[i][param_name] * task_weights[i]  # ğŸ‘ˆ åŠ¨æ€æƒé‡ç”Ÿæ•ˆï¼
            if weighted_sum is None:
                weighted_sum = delta_weighted
            else:
                weighted_sum += delta_weighted
        tau[param_name] = weighted_sum

    # Step 3: Î¸* = Î¸â‚€ + Î» * Ï„
    final_params = {}
    for param_name in tqdm(param_names, desc="Applying scaling and merging"):
        final_params[param_name] = theta_0[param_name] + scaling_lambda * tau[param_name]

    # --- 5. åŠ è½½åˆå¹¶åçš„å‚æ•°å¹¶ä¿å­˜æ¨¡å‹ ---
    print("Loading merged parameters...")
    base_model = AutoModelForCausalLM.from_pretrained(base_model_name, device_map="cpu", trust_remote_code=True)
    final_params_casted = {
        k: v.to(base_model.state_dict()[k].dtype)
        for k, v in final_params.items()
    }
    base_model.load_state_dict(final_params_casted)

    # --- 6. ä¿å­˜æ¨¡å‹ ---
    save_model_and_tokenizer(base_model, base_model_name, output_path)

    # --- 7. æ¸…ç†å†…å­˜ ---
    del base_model, model_params_list, deltas, theta_0, tau, final_params, final_params_casted
    gc.collect()
    torch.cuda.empty_cache()

    print(f"âœ… LORE-MERGING with dynamic weights completed! Model saved to: {output_path}")

def emr_merge_models(
    base_model_name: str,
    model_names_list: List[str],
    task_weights: Optional[List[float]] = None,  # ğŸ‘ˆ æ–°å¢å‚æ•°ï¼šä»»åŠ¡æƒé‡
    output_path: str = "./emr_merged_model"
):
    """
    ã€æ”¯æŒåŠ¨æ€æ¯”ä¾‹çš„ EMR-Mergingã€‘
    é€šè¿‡ task_weights åŠ¨æ€è°ƒèŠ‚æ¯ä¸ªä»»åŠ¡çš„è´¡çŒ®æ¯”ä¾‹ã€‚

    Args:
        base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_names_list (list[str]): éœ€è¦åˆå¹¶çš„å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„åˆ—è¡¨ã€‚
        task_weights (list[float], optional): æ¯ä¸ªä»»åŠ¡çš„æƒé‡ã€‚è‹¥ä¸º Noneï¼Œåˆ™ä½¿ç”¨å¹³å‡æƒé‡ã€‚
        output_path (str): åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚
    """
    print("-" * 80)
    n_models = len(model_names_list)
    print(f"Starting EMR-Merging for {n_models} models")

    if task_weights is None:
        task_weights = [1.0 / n_models] * n_models  # é»˜è®¤å¹³å‡
        print("â†’ Using equal weights for all tasks")
    else:
        assert len(task_weights) == n_models, "task_weights é•¿åº¦å¿…é¡»ç­‰äºæ¨¡å‹æ•°é‡"
        # å¯é€‰ï¼šå½’ä¸€åŒ–æƒé‡ï¼ˆéå¿…é¡»ï¼‰
        total_weight = sum(task_weights)
        task_weights = [w / total_weight for w in task_weights]
        print(f"â†’ Using custom weights: {task_weights}")

    # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
    )
    base_model.eval()
    params_base = base_model.state_dict()

    fine_tuned_params_list = []
    for model_name in model_names_list:
        print(f"Loading fine-tuned model: {model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
        )
        model.eval()
        fine_tuned_params_list.append(model.state_dict())
        del model
    gc.collect()

    # --- 2. è®¡ç®—ä»»åŠ¡å‘é‡ ---
    print("Computing task vectors...")
    task_vectors = []
    param_names = list(params_base.keys())

    for i, ft_params in enumerate(fine_tuned_params_list):
        print(f"Computing task vector {i+1}/{n_models}")
        task_vector = {}
        for param_name in tqdm(param_names, desc=f"TV {i+1}", leave=False):
            if param_name in ft_params:
                base_tensor = params_base[param_name]
                ft_tensor = ft_params[param_name]
                task_vector[param_name] = ft_tensor - base_tensor
        task_vectors.append(task_vector)

    # --- 3. EMRåˆå¹¶ç®—æ³• ---
    print("Applying EMR merging algorithm...")

    # æ­¥éª¤1: è®¡ç®—å¹³å‡å‚æ•°ï¼ˆç”¨äºæ–¹å‘æ ‡å¿—ï¼‰
    sum_param = {}
    for param_name in tqdm(param_names, desc="Computing average parameters", leave=False):
        if param_name in task_vectors[0]:
            param_values = []
            for tv in task_vectors:
                if param_name in tv:
                    param_values.append(tv[param_name])
            if param_values:
                sum_param[param_name] = torch.stack(param_values, 0).mean(0)

    # æ­¥éª¤2-6: EMRæ ¸å¿ƒç®—æ³•
    vector_unified = {}
    scales = torch.zeros(n_models)
    masks = {}

    for param_name in tqdm(param_names, desc="EMR core algorithm", leave=False):
        if param_name not in sum_param:
            continue

        masks[param_name] = []
        flag = (sum_param[param_name] > 0) * 2 - 1  # æ–¹å‘æ ‡å¿—
        param_max = torch.zeros_like(task_vectors[0][param_name])

        for m in range(n_models):
            if param_name in task_vectors[m]:
                param = task_vectors[m][param_name]
                mask = (param * flag) > 0
                masks[param_name].append(mask)
                param_abs = torch.abs(mask * param)
                param_max = torch.where(param_abs > param_max, param_abs, param_max)
                scales[m] += torch.mean(torch.abs(param))

        vector_unified[param_name] = param_max * flag

    # æ­¥éª¤6: è®¡ç®—é‡ç¼©æ”¾å› å­
    print("Computing rescaling factors...")
    new_scales = torch.zeros(n_models)

    for m in range(n_models):
        for param_name in vector_unified:
            if param_name in masks and len(masks[param_name]) > m:
                p = vector_unified[param_name] * masks[param_name][m]
                new_scales[m] += torch.mean(torch.abs(p))

    rescalers = torch.where(new_scales > 0, scales / new_scales, torch.ones_like(scales))

    # --- 4. åº”ç”¨EMRåˆå¹¶ç»“æœï¼ˆä½¿ç”¨åŠ¨æ€æƒé‡ï¼‰---
    print("Applying EMR merged parameters with dynamic weights...")
    merged_params = base_model.state_dict()

    for param_name in tqdm(param_names, desc="Applying EMR results", leave=False):
        if param_name in vector_unified:
            base_tensor = params_base[param_name]
            weighted_task_vector = torch.zeros_like(base_tensor)

            for m in range(n_models):
                if param_name in masks and len(masks[param_name]) > m:
                    task_vector_recon = vector_unified[param_name] * masks[param_name][m] * rescalers[m]
                    weighted_task_vector += task_weights[m] * task_vector_recon  # ğŸ‘ˆ åŠ¨æ€æƒé‡ï¼

            merged_params[param_name] = base_tensor + weighted_task_vector

    # --- 5. ä¿å­˜æ¨¡å‹ ---
    print("Loading merged parameters...")
    base_model.load_state_dict(merged_params)
    save_model_and_tokenizer(base_model, base_model_name, output_path)

    # --- 6. æ¸…ç†å†…å­˜ ---
    del base_model, fine_tuned_params_list, task_vectors, vector_unified, masks, merged_params
    gc.collect()
    torch.cuda.empty_cache()

    print(f"âœ… EMR-Merging with dynamic weights completed! Model saved to: {output_path}")


import numpy as np

def normalize(v: np.ndarray, eps: float = 1e-8):
    norm_v = np.linalg.norm(v)
    if norm_v > eps:
        v = v / norm_v
    return v

def lerp(t: float, v0: np.ndarray, v1: np.ndarray) -> np.ndarray:
    return (1 - t) * v0 + t * v1

def slerp(
    t: float,
    v0: torch.Tensor,
    v1: torch.Tensor,
    DOT_THRESHOLD: float = 0.9995,
    eps: float = 1e-8,
) -> torch.Tensor:
    """
    Spherical Linear Interpolation for PyTorch tensors.
    """
    # è½¬ä¸º numpy å¤„ç†
    v0_np = v0.detach().cpu().float().numpy()
    v1_np = v1.detach().cpu().float().numpy()
    v0_copy, v1_copy = np.copy(v0_np), np.copy(v1_np)

    # å½’ä¸€åŒ–æ–¹å‘
    v0_norm = normalize(v0_np, eps)
    v1_norm = normalize(v1_np, eps)

    dot = np.sum(v0_norm * v1_norm)

    # è‹¥æ¥è¿‘å…±çº¿ï¼Œé€€åŒ–ä¸ºçº¿æ€§æ’å€¼
    if np.abs(dot) > DOT_THRESHOLD:
        res = lerp(t, v0_copy, v1_copy)
    else:
        theta_0 = np.arccos(np.clip(dot, -1.0, 1.0))
        sin_theta_0 = np.sin(theta_0)
        theta_t = theta_0 * t
        sin_theta_t = np.sin(theta_t)
        s0 = np.sin(theta_0 - theta_t) / (sin_theta_0 + eps)
        s1 = sin_theta_t / (sin_theta_0 + eps)
        res = s0 * v0_copy + s1 * v1_copy

    # è½¬å› torch tensorï¼Œä¿ç•™åŸè®¾å¤‡å’Œ dtype
    res_tensor = torch.from_numpy(res).to(v0.dtype).to(v0.device)
    return res_tensor

# --- ä¸»å‡½æ•°ï¼šä»¿ç…§ weighted_average_models æ¥å£ ---

def slerp_merge_models(
    model_name_a: str,
    model_name_b: str,
    t: float,
    output_path: str = "./slerp_merged_model"
):
    """
    ä½¿ç”¨çƒé¢çº¿æ€§æ’å€¼ï¼ˆSlerpï¼‰åˆå¹¶ä¸¤ä¸ªæ¨¡å‹ï¼š
        result = slerp(t, model_a, model_b)
    å½“ t=0 æ—¶å®Œå…¨ç­‰äº model_aï¼Œt=1 æ—¶å®Œå…¨ç­‰äº model_bã€‚

    Args:
        model_name_a (str): èµ·å§‹æ¨¡å‹è·¯å¾„
        model_name_b (str): ç›®æ ‡æ¨¡å‹è·¯å¾„
        t (float): æ’å€¼ç³»æ•°ï¼ŒèŒƒå›´ [0, 1]
        output_path (str): åˆå¹¶åæ¨¡å‹ä¿å­˜è·¯å¾„
    """
    print("-" * 80)
    print(f"Starting SLERP merge with t = {t}")

    # åŠ è½½æ¨¡å‹
    print("Loading model A...")
    model_a = AutoModelForCausalLM.from_pretrained(
        model_name_a, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
    )
    model_a.eval()
    params_a = model_a.state_dict()

    print("Loading model B...")
    model_b = AutoModelForCausalLM.from_pretrained(
        model_name_b, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
    )
    model_b.eval()
    params_b = model_b.state_dict()

    # æ‰§è¡Œ SLERP åˆå¹¶
    merged_params = {}
    param_names = list(params_a.keys())

    for param_name in tqdm(param_names, desc=f"SLERP (t={t})"):
        if param_name in params_b:
            tensor_a = params_a[param_name]
            tensor_b = params_b[param_name].to(tensor_a.device, dtype=tensor_a.dtype)
            # ğŸ‘‡ æ ¸å¿ƒï¼šä½¿ç”¨ SLERP æ›¿ä»£çº¿æ€§æ’å€¼
            merged_params[param_name] = slerp(t, tensor_a, tensor_b)
        else:
            # è‹¥ model_b æ²¡æœ‰è¯¥å‚æ•°ï¼Œç›´æ¥ä¿ç•™ model_a çš„
            merged_params[param_name] = params_a[param_name]

    # åŠ è½½åˆå¹¶åå‚æ•°
    print("Loading merged parameters...")
    model_a.load_state_dict(merged_params)

    # ä¿å­˜æ¨¡å‹
    save_model_and_tokenizer(model_a, model_name_a, output_path)

    # æ¸…ç†å†…å­˜
    del model_a, model_b, params_a, params_b, merged_params
    gc.collect()
    torch.cuda.empty_cache()

    print(f"âœ… SLERP merging completed! Model saved to: {output_path}")

def ties_merge_models_TA(base_model_name, model_names_list, density, weights=None,  output_path=None,scaling_lambda=1.0):
    """
    ä½¿ç”¨æ ‡å‡† TIES-Merging (Trim, Elect Sign & Merge) æ–¹æ³•åˆå¹¶å¤šä¸ªæ¨¡å‹ã€‚
    æ­¤å®ç°éµå¾ª mergekit åº“çš„æ ‡å‡†ç®—æ³•ï¼Œä½¿ç”¨å±€éƒ¨é˜ˆå€¼å’ŒåŠ æƒç¬¦å·ä¸€è‡´æ€§ã€‚

    Args:
        base_model_name (str): åŸºç¡€é¢„è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ã€‚
        model_names_list (list[str]): éœ€è¦åˆå¹¶çš„å¤šä¸ªå¾®è°ƒæ¨¡å‹çš„è·¯å¾„åˆ—è¡¨ã€‚
        density (float): Trim æ­¥éª¤ä¸­ä¿ç•™çš„å‚æ•°å¯†åº¦ã€‚æ¯ä¸ªtensorç‹¬è‡ªè®¡ç®—é˜ˆå€¼ã€‚
        weights(list[float],optinal):æ¯ä¸ªæ¨¡å‹æƒé‡ã€‚
        scaling_lambda (float): æœ€ç»ˆåˆå¹¶ä»»åŠ¡å‘é‡çš„ç¼©æ”¾ç³»æ•° Î»ã€‚
        output_path (str, optional): åˆå¹¶åæ¨¡å‹çš„ä¿å­˜è·¯å¾„ã€‚å¦‚æœä¸º Noneï¼Œåˆ™è¿”å›åˆå¹¶åçš„æ¨¡å‹ã€‚
    
    Returns:
        AutoModelForCausalLM: åˆå¹¶åçš„æ¨¡å‹
    """
    print("-" * 80)
    print(f"Starting Standard TIES-Merging with density={density} and scaling_lambda={scaling_lambda}")
    
    if weights is None:
        weights = [1.0] * len(model_names_list)
    elif len(weights) != len(model_names_list):
        raise ValueError("Weights list length must match model_names_list length")

    # --- 1. åŠ è½½æ‰€æœ‰æ¨¡å‹ ---
    print("Loading base model...")
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
    )
    base_model.eval()
    params_base = base_model.state_dict()

    fine_tuned_params_list = []
    for model_name in model_names_list:
        print(f"Loading fine-tuned model: {model_name}")
        model = AutoModelForCausalLM.from_pretrained(
            model_name, torch_dtype=torch.bfloat16, device_map="cpu", trust_remote_code=True
        )
        model.eval()
        fine_tuned_params_list.append(model.state_dict())
        del model
    gc.collect()

    # --- 2. é€ä¸ªå‚æ•°è¿›è¡Œæ ‡å‡† TIES åˆå¹¶ ---
    final_merged_task_vector = {}
    param_names = list(params_base.keys())

    for param_name in tqdm(param_names, desc="TIES-Merging with Local Trim"):
        base_tensor = params_base[param_name]
        
        # --- è®¡ç®—å½“å‰å‚æ•°çš„ä»»åŠ¡å‘é‡ ---
        tvs_for_param = []
        for i, ft_params in enumerate(fine_tuned_params_list):
            delta = ft_params[param_name] - base_tensor
            tvs_for_param.append({
                'delta': delta,
                'weight': weights[i],
                'density': density
            })

        # --- Step 1: Trim (Local Threshold per tensor) ---
        trimmed_tvs_for_param = []
        for tv_info in tvs_for_param:
            delta = tv_info['delta']
            density = tv_info['density']
            
            if delta.ndim == 0 or density >= 1.0:  # Skip scalar tensors or no trimming
                trimmed_tvs_for_param.append(delta)
                continue

            # ä½¿ç”¨å±€éƒ¨é˜ˆå€¼è¿›è¡Œä¿®å‰ªï¼ˆæŒ‰å¼ é‡ç‹¬ç«‹è®¡ç®—ï¼‰
            k = int(density * delta.numel())
            if k <= 0:
                trimmed_tvs_for_param.append(torch.zeros_like(delta))
                continue
                
            # è®¡ç®—å±€éƒ¨é˜ˆå€¼
            flat_delta = delta.abs().view(-1)
            if flat_delta.device.type == "cpu":
                flat_delta = flat_delta.float()
            
            # æ‰¾åˆ°ç¬¬ k å¤§çš„å€¼ä½œä¸ºé˜ˆå€¼
            topk_values, _ = torch.topk(flat_delta, k, largest=True)
            threshold = topk_values[-1] if k > 0 else 0.0
            
            # åº”ç”¨é˜ˆå€¼æ©ç 
            mask = delta.abs() >= threshold
            trimmed_tvs_for_param.append(delta * mask)
        
        # --- Step 2: Elect Sign (Weighted Consensus) ---
        # æŒ‰ç…§ mergekit åº“çš„æ ‡å‡†å®ç°åŠ æƒç¬¦å·ä¸€è‡´æ€§
        deltas = torch.stack(trimmed_tvs_for_param, dim=0)
        weights_tensor = torch.tensor(weights, dtype=deltas.dtype, device=deltas.device)
        
        # è®¡ç®—åŠ æƒå’Œæ¥ç¡®å®šå¤šæ•°ç¬¦å·
        weighted_deltas = deltas * weights_tensor.view(-1, *([1] * (deltas.ndim - 1)))
        sign_weight = weighted_deltas.sum(dim=0)
        majority_sign = (sign_weight >= 0).float() * 2 - 1
        
        # --- Step 3: Merge with Sign Consensus ---
        # åˆ›å»ºç¬¦å·ä¸€è‡´æ€§æ©ç 
        signs = deltas.sign()
        consensus_mask = (signs == majority_sign).float()
        
        # åº”ç”¨æƒé‡å’Œç¬¦å·ä¸€è‡´æ€§
        weighted_consensus_deltas = weighted_deltas * consensus_mask
        final_sum = weighted_consensus_deltas.sum(dim=0)
        
        # è®¡ç®—å½’ä¸€åŒ–å› å­
        weight_mask_sum = (weights_tensor.view(-1, *([1] * (deltas.ndim - 1))) * consensus_mask).sum(dim=0)
        weight_mask_sum[weight_mask_sum.abs() < 1e-8] = 1.0  # é¿å…é™¤ä»¥é›¶
        
        # å½’ä¸€åŒ–
        merged_tensor = final_sum / weight_mask_sum
        final_merged_task_vector[param_name] = merged_tensor

    # æ¸…ç†å†…å­˜
    del fine_tuned_params_list
    gc.collect()

    # --- 3. å°†åˆå¹¶åçš„ä»»åŠ¡å‘é‡åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹ ---
    print("Applying merged task vector to the base model...")
    merged_params = base_model.state_dict()
    for param_name, tensor in final_merged_task_vector.items():
        merged_params[param_name] += (scaling_lambda * tensor).to(merged_params[param_name].device)

    base_model.load_state_dict(merged_params)
    
    # --- 4. ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼‰ ---
    if output_path:
        print(f"Saving merged model to {output_path}")
        base_model.save_pretrained(output_path)
        # å¦‚æœåŸºç¡€æ¨¡å‹æœ‰ tokenizerï¼Œä¹Ÿä¿å­˜å®ƒ
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            tokenizer.save_pretrained(output_path)
        except Exception as e:
            print(f"Warning: Could not save tokenizer: {e}")

    # æœ€ç»ˆæ¸…ç†
    del final_merged_task_vector, merged_params
    gc.collect()
    torch.cuda.empty_cache()
    
    return base_model