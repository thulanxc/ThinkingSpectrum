import os
import json
import argparse

# å¯¼å…¥ transformers åº“
try:
    from transformers import AutoTokenizer
except ImportError:
    print("é”™è¯¯: 'transformers' åº“æœªæ‰¾åˆ°ã€‚")
    print("è¯·è¿è¡Œ 'pip install transformers torch sentencepiece' è¿›è¡Œå®‰è£…ã€‚")
    exit()

# --- é…ç½® ---
# æŒ‡å®šè¦ä½¿ç”¨çš„ Hugging Face Tokenizer æ¨¡å‹
TOKENIZER_MODEL = "Qwen/Qwen2.5-Math-7B-Instruct"

def process_jsonl_file(file_path, tokenizer):
    """
    è®¡ç®—å•ä¸ª .jsonl æ–‡ä»¶ä¸­æ‰€æœ‰ score çš„å¹³å‡æ­£ç¡®ç‡ï¼Œ
    å¹¶è®¡ç®— 'code' å­—æ®µä¸­æ‰€æœ‰å›ç­”çš„å¹³å‡ Token æ•°é‡ã€‚
    """
    # ç”¨äºè®¡ç®—å‡†ç¡®ç‡çš„å˜é‡
    total_predictions = 0
    correct_predictions = 0
    
    # ç”¨äºè®¡ç®—Tokenæ•°çš„å˜é‡
    total_tokens = 0
    total_responses = 0

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                try:
                    data = json.loads(line.strip())
                    
                    # --- 1. è®¡ç®—å‡†ç¡®ç‡ (åŸæœ‰é€»è¾‘) ---
                    if 'score' in data and isinstance(data['score'], list):
                        scores = data['score']
                        total_predictions += len(scores)
                        # bool(True) æ˜¯ 1, bool(False) æ˜¯ 0ï¼Œå¯ä»¥ç›´æ¥æ±‚å’Œ
                        correct_predictions += sum(bool(s) for s in scores)
                    else:
                        print(f"   - è­¦å‘Š: æ–‡ä»¶ '{os.path.basename(file_path)}' çš„ç¬¬ {i+1} è¡Œç¼ºå°‘ 'score' åˆ—è¡¨ã€‚")

                    # --- 2. æ–°å¢: è®¡ç®—Tokenæ•° ---
                    if 'code' in data and isinstance(data['code'], list):
                        for response_text in data['code']:
                            # ç¡®ä¿å…ƒç´ æ˜¯å­—ç¬¦ä¸²ç±»å‹å†è¿›è¡Œtokenize
                            if isinstance(response_text, str):
                                token_ids = tokenizer.encode(response_text)
                                total_tokens += len(token_ids)
                                total_responses += 1
                            else:
                                print(f"   - è­¦å‘Š: æ–‡ä»¶ '{os.path.basename(file_path)}' çš„ç¬¬ {i+1} è¡Œ 'code' åˆ—è¡¨ä¸­åŒ…å«éå­—ç¬¦ä¸²å…ƒç´ ã€‚")
                    else:
                        print(f"   - è­¦å‘Š: æ–‡ä»¶ '{os.path.basename(file_path)}' çš„ç¬¬ {i+1} è¡Œç¼ºå°‘ 'code' åˆ—è¡¨ã€‚")

                except json.JSONDecodeError:
                    print(f"   - è­¦å‘Š: æ–‡ä»¶ '{os.path.basename(file_path)}' çš„ç¬¬ {i+1} è¡Œ JSON æ ¼å¼é”™è¯¯ã€‚")
                    continue
    
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ '{file_path}' æœªæ‰¾åˆ°ã€‚")
        return None, None, None, None

    # --- 3. è®¡ç®—æœ€ç»ˆç»“æœ ---
    # è®¡ç®—å‡†ç¡®ç‡ï¼Œé¿å…é™¤ä»¥é›¶
    accuracy = (correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0.0
    # è®¡ç®—å¹³å‡Tokenæ•°ï¼Œé¿å…é™¤ä»¥é›¶
    avg_tokens = total_tokens / total_responses if total_responses > 0 else 0.0

    return accuracy, correct_predictions, total_predictions, avg_tokens

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºè§£æå‚æ•°å¹¶éå†æ–‡ä»¶å¤¹ã€‚
    """
    parser = argparse.ArgumentParser(
        description="è®¡ç®—æ–‡ä»¶å¤¹ä¸­æ‰€æœ‰ .jsonl æ–‡ä»¶çš„â€˜æ‹å¹³åâ€™çš„å¹³å‡å‡†ç¡®ç‡å’Œå¹³å‡Tokenæ•°ã€‚"
    )
    parser.add_argument(
        "folder_path",
        type=str,
        nargs='?', # ä½¿å‚æ•°å˜ä¸ºå¯é€‰
        default='.', # é»˜è®¤å€¼ä¸ºå½“å‰æ–‡ä»¶å¤¹
        help="åŒ…å« .jsonl æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ (é»˜è®¤ä¸ºå½“å‰æ–‡ä»¶å¤¹)"
    )
    args = parser.parse_args()
    
    target_folder = args.folder_path

    if not os.path.isdir(target_folder):
        print(f"âŒ é”™è¯¯: '{target_folder}' ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚")
        return

    # --- æ–°å¢: åŠ è½½ Tokenizer ---
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½ Tokenizer: '{TOKENIZER_MODEL}'...")
    try:
        # trust_remote_code=True æ˜¯æŸäº›æ¨¡å‹ï¼ˆå¦‚Qwenï¼‰æ‰€å¿…éœ€çš„
        tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_MODEL, trust_remote_code=True)
        print("Tokenizer åŠ è½½æˆåŠŸï¼\n")
    except Exception as e:
        print(f"âŒ é”™è¯¯: åŠ è½½ Tokenizer å¤±è´¥ã€‚")
        print(f"   è¯·ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼Œå¹¶å·²æ­£ç¡®å®‰è£…æ‰€éœ€åº“ (pip install transformers torch sentencepiece)ã€‚")
        print(f"   è¯¦ç»†é”™è¯¯: {e}")
        return

    print(f"ğŸ” æ­£åœ¨æ‰«ææ–‡ä»¶å¤¹: '{os.path.abspath(target_folder)}'...\n")

    jsonl_files = [f for f in os.listdir(target_folder) if f.endswith('.jsonl')]

    if not jsonl_files:
        print("åœ¨è¯¥æ–‡ä»¶å¤¹ä¸­æœªæ‰¾åˆ° .jsonl æ–‡ä»¶ã€‚")
        return
        
    for filename in sorted(jsonl_files):
        file_path = os.path.join(target_folder, filename)
        accuracy, correct, total, avg_tokens = process_jsonl_file(file_path, tokenizer)
        
        if accuracy is not None:
            print(f"ğŸ“„ æ–‡ä»¶: {filename}")
            print(f"   => å¹³å‡å‡†ç¡®ç‡: {accuracy:.2f}% ({correct}/{total} ä¸ªé¢„æµ‹æ­£ç¡®)")
            print(f"   => å¹³å‡Tokenæ•°: {avg_tokens:.0f}\n") # æ–°å¢è¾“å‡º

if __name__ == "__main__":
    main()