# æ–‡ä»¶å: run_experiment.py
import os
import stat
from datetime import datetime
import model_merger  # ç¡®ä¿ model_merger.py åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ä¸‹

# è®¾ç½®ä½¿ç”¨çš„GPU
os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"

# ======================================================================================
# --- âš™ï¸ å®éªŒæ§åˆ¶é¢æ¿ (åœ¨è¿™é‡Œä¿®æ”¹) ---
# ======================================================================================
EXPERIMENT_CONTROL_PANEL = {
    # "ties_merge_ta": [(80,[0.2,0.8]),(80,[0.4,0.6]),(80,[0.6,0.4]),(80,[0.8,0.2])],#ok
    "lore_merge": [(2,[0.2,0.8]),(2,[0.4,0.6]),(2,[0.6,0.4]),(2,[0.8,0.2])],
    # "emr": [[0.2,0.8],[0.4,0.6],[0.6,0.4],[0.8,0.2]],
    # "emr": [[0.2,0.8]], 

    # "dare_merge_TA": [(0.2,0.2),(0.2, 0.4),(0.2,0.6),(0.2,0.8)],# ok

    # 'surgical_merge': [1,2,5,10,20,50],
    # "avg_override_top_k_thinking": [1,2,5,10,20,50],
    # --- DARE åˆå¹¶æ–¹æ³•çš„æ§åˆ¶å¼€å…³ ---
    # å‚æ•°æ ¼å¼: [(drop_rate, scaling_lambda), ...]
    # drop_rate: éšæœºä¸¢å¼ƒç‡ p (è®ºæ–‡æ¨è 0.9 æˆ– 0.99)
    # scaling_lambda: ä»»åŠ¡ç®—æœ¯çš„åˆå¹¶ç³»æ•° Î» (é€šå¸¸è®¾ä¸º 0.5 åˆ° 1.0 ä¹‹é—´)
    # "dare_merge": [(0.1,0.5),(0.2, 0.5),(0.3,0.5),(0.4,0.5),(0.8,0.5)],
    # --- TIES-Merging åˆå¹¶æ–¹æ³•çš„æ§åˆ¶å¼€å…³ ---
    # å‚æ•°æ ¼å¼: [(top_k_percentage, scaling_lambda), ...]
    # top_k_percentage: Trim æ­¥éª¤ä¿ç•™çš„ top k% å‚æ•° (è®ºæ–‡ä¸­ k=20 æ•ˆæœä¸é”™)
    # scaling_lambda: ä»»åŠ¡å‘é‡çš„ç¼©æ”¾ç³»æ•° Î» (è®ºæ–‡ä¸­ Î»=1)
    # "ties_merge": [(95,1.0),(90,1.0),(80,1.0),(70,1.0),(50,1.0)], # ç¤ºä¾‹: è¿è¡Œä¸€æ¬¡ TIES åˆå¹¶
    # "weighted_average":[0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],
    # "slerp":[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],
    # "top_k_avg_keep_thinking": [],
    # "top_k_avg_keep_instruct": [1,2,5,10,20,50],
    # "avg_override_top_k_thinking": []
    # "weighted_average":[0,1],
}

# ======================================================================================
# --- ğŸ“‚ å…¨å±€è·¯å¾„ä¸ä»»åŠ¡é…ç½® (åœ¨è¿™é‡Œä¿®æ”¹) ---
# ======================================================================================

# --- å®šä¹‰æ‰€æœ‰éœ€è¦è¿›è¡Œåˆå¹¶å®éªŒçš„æ¨¡å‹å¯¹ ---
# !!! é‡è¦: DARE å’Œ TIES åˆå¹¶éœ€è¦ä¸€ä¸ªå…±åŒçš„ "base" åŸºç¡€æ¨¡å‹ !!!
MODEL_PAIRS_TO_MERGE = [
    {
    "base": "../../../models/Qwen3-30B-A3B",
    "instruct": "../../../models/Qwen3-30B-A3B-Instruct-2507",
    "thinking": "../../../models/Qwen3-30B-A3B-Thinking-2507"
    },
    # {
    # "base": "Qwen/Qwen3-4B",
    # "instruct": "../../../models/Qwen3-4B-Instruct-2507",
    # "thinking": "../../../models/Qwen3-4B-Thinking-2507"
    # },   
    # {
    # "base": "Qwen/Qwen2-Math-1.5B",
    # "instruct": "Qwen/Qwen2.5-Math-1.5B-Instruct",
    # "thinking": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
    # },
    # {
    # "base": "Qwen/Qwen2-Math-7B",
    # "instruct": "Qwen/Qwen2.5-Math-7B-Instruct",
    # "thinking": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B"
    # }
]

# --- å…¨å±€è·¯å¾„é…ç½® ---
# åˆå¹¶åæ¨¡å‹çš„é¡¶çº§è¾“å‡ºç›®å½•
BASE_MODEL_OUTPUT_DIR = "../../../models" 
# å­˜æ”¾æ¯æ¬¡è¿è¡Œç”Ÿæˆçš„è„šæœ¬çš„ç›®å½•
AUTOMATED_RUNS_DIR = "automated_runs"
# ã€æ–°å¢ã€‘æŒ‡å®šä½ çš„æ•°æ®é›†æ–‡ä»¶å¤¹ç›¸å¯¹äºæœ¬è„šæœ¬çš„ä½ç½®
# å‡è®¾ä½ çš„ data æ–‡ä»¶å¤¹å’Œ run_experiment.py åœ¨åŒä¸€çº§ç›®å½•
DATA_DIR_PATH_IN_PROJECT = "../../data" 

# --- è¯„ä¼°ä»»åŠ¡é…ç½® ---
EVAL_TASKS = [
    "aime24 10"
    # "gpqa_diamond 1"
]

# ======================================================================================
# --- ğŸ“œ è„šæœ¬æ¨¡æ¿ (è·¯å¾„å·²æ ¡å‡†ï¼Œæ— éœ€ä¿®æ”¹) ---
# ======================================================================================
RUN_EVAL_SH_TEMPLATE = """#!/bin/bash
set -ex
export CUDA_VISIBLE_DEVICES="0,1,2,3"
PROMPT_TYPE="qwen25-no-cot"

MODELS=(
{model_paths_string}
)

TASKS=(
{tasks_string}
)

run_evaluation() {{
    local MODEL_NAME_OR_PATH=$1
    local DATA_NAME=$2
    local N_SAMPLING=$3
    local OUTPUT_DIR

    echo "================================================================"
    echo "STARTING EVALUATION FOR: ${{MODEL_NAME_OR_PATH}}"
    echo "ON DATA: ${{DATA_NAME}} with N_SAMPLING: ${{N_SAMPLING}}"
    echo "================================================================"
    
    CLEAN_MODEL_NAME=$(basename "${{MODEL_NAME_OR_PATH}}")
    OUTPUT_DIR="${{CLEAN_MODEL_NAME}}"

    # ã€é‡è¦ä¿®æ”¹ã€‘æ˜¾å¼ä¼ å…¥æ•°æ®é›†çš„è·¯å¾„ï¼Œç¡®ä¿è„šæœ¬èƒ½æ‰¾åˆ°æ•°æ®
    TOKENIZERS_PARALLELISM=false \\
    python3 -u ../../math_eval.py \\
        --model_name_or_path "${{MODEL_NAME_OR_PATH}}" \\
        --data_name "${{DATA_NAME}}" \\
        --data_dir "{data_dir_path}" \\
        --output_dir "${{OUTPUT_DIR}}" \\
        --split "test" \\
        --prompt_type "${{PROMPT_TYPE}}" \\
        --num_test_sample -1 \\
        --seed 0 \\
        --temperature 0.6 \\
        --n_sampling "${{N_SAMPLING}}" \\
        --top_p 0.95 \\
        --top_k 20 \\
        --start 0 \\
        --end -1 \\
        --use_vllm \\
        --save_outputs \\
        --overwrite \\
        --max_tokens_per_call 39512 \\
        --presence_penalty 0 \\
        --frequency_penalty 0
}}

for model in "${{MODELS[@]}}"; do
    for task in "${{TASKS[@]}}"; do
        read -r data_names n_sampling <<< "$task"
        run_evaluation "$model" "$data_names" "$n_sampling"
    done
done

echo "All evaluation tasks are complete."
"""

CHECK_ACCURACY_PY_TEMPLATE = """import os, json
from transformers import AutoTokenizer
# Auto-generated on {generation_time} to check multiple models.

MODELS_TO_EVALUATE = [
{model_configs_string}
]
DEFAULT_BENCHMARKS = [
{benchmarks_string}
]

def process_file(file_path, tokenizer):
    correct, total_preds, total_tokens, total_responses = 0, 0, 0, 0
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                if 'score' in data and isinstance(data['score'], list):
                    correct += sum(bool(s) for s in data['score'])
                    total_preds += len(data['score'])
                if 'code' in data and isinstance(data['code'], list):
                    for resp in data['code']:
                        if isinstance(resp, str): total_tokens += len(tokenizer.encode(resp)); total_responses += 1
    except FileNotFoundError: return None
    acc = (correct / total_preds) * 100 if total_preds > 0 else 0
    tokens = total_tokens / total_responses if total_responses > 0 else 0
    return acc, correct, total_preds, tokens

def main():
    print("="*60 + "\\nStarting Accuracy Evaluation\\n" + "="*60)
    for config in MODELS_TO_EVALUATE:
        print("\\n--- Model: '{{}}' ---".format(config['model_name']))
        try:
            tokenizer = AutoTokenizer.from_pretrained(config['tokenizer_path'], trust_remote_code=True)
        except Exception as e:
            print("     âŒ ERROR: Failed to load tokenizer from {{}}. Details: {{}}. Skipping.".format(config['tokenizer_path'], e))
            continue
        for benchmark in DEFAULT_BENCHMARKS:
            bench_path = os.path.join(config['results_path'], benchmark)
            print("     â–¶ï¸ Benchmark: {{}}".format(benchmark))
            if not os.path.isdir(bench_path):
                print("         â—ï¸ SKIPPED: Directory not found: {{}}".format(bench_path))
                continue
            
            jsonl_files = [f for f in os.listdir(bench_path) if f.endswith('.jsonl')]
            if not jsonl_files:
                print("         â—ï¸ SKIPPED: No .jsonl result files found in directory.")
                continue

            for fname in sorted(jsonl_files):
                fpath = os.path.join(bench_path, fname)
                res = process_file(fpath, tokenizer)
                if res:
                    accuracy, correct, total, avg_tokens = res
                    print("         âœ”ï¸ {{:<40}} | Accuracy: {{:>6.2f}}% ({{:>3}}/{{:<3}}) | Avg Length: {{:.0f}} tokens".format(
                        fname, accuracy, correct, total, avg_tokens
                    ))
if __name__ == "__main__": main()
"""

# ======================================================================================
# --- ğŸ¤– ä¸»æ‰§è¡Œé€»è¾‘ (è·¯å¾„å·²æ ¡å‡†ï¼Œæ— éœ€ä¿®æ”¹) ---
# ======================================================================================
def main():
    active_method, params_to_run = None, None
    for method, params in EXPERIMENT_CONTROL_PANEL.items():
        if params:
            if active_method is not None:
                print("âŒ Error: Multiple experiments are activated in the control panel. Please ensure that parameters are provided for only one method at a time.")
                return
            active_method = method
            params_to_run = params

    if not active_method:
        print("ğŸ¤· Info: No experiment to run was found in the control panel. Exiting script.")
        return

    print(f"ğŸš€ Preparing to execute experiment: Method='{active_method}', Parameters={params_to_run}")

    # ã€é‡è¦ä¿®æ”¹ã€‘æ–‡ä»¶å¤¹å‘½åè§„åˆ™: æ–¹æ³•å_æ—¥æœŸ_æ—¶é—´
    run_folder_name = f"{active_method}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    current_run_dir = os.path.join(AUTOMATED_RUNS_DIR, run_folder_name)
    os.makedirs(current_run_dir, exist_ok=True)
    print(f"Created main directory for this run: {current_run_dir}")

    generated_models_info = []

    for model_pair in MODEL_PAIRS_TO_MERGE:
        instruct_model_path = model_pair["instruct"]
        thinking_model_path = model_pair["thinking"]
        base_model_path = model_pair.get("base")

        if (active_method == 'dare_merge' or active_method == 'ties_merge') and not base_model_path:
            print(f"âŒ Error: {active_method.upper()} merge requires a 'base' model. Skipping model pair: {os.path.basename(instruct_model_path)} and {os.path.basename(thinking_model_path)}")
            continue

        print("\n" + "#"*80)
        print(f"## Processing model pair: Instruct='{os.path.basename(instruct_model_path)}', Thinking='{os.path.basename(thinking_model_path)}'")
        if base_model_path:
            print(f"## Base model: Base='{os.path.basename(base_model_path)}'")
        print("#"*80)

        instruct_name = os.path.basename(instruct_model_path)
        thinking_name = os.path.basename(thinking_model_path)

    for param_value in params_to_run:
        print("\n" + "="*80 + f"\nâ–¶ï¸ Processing parameters: {param_value}\n" + "="*80)
        model_output_path, merge_func = "", None
        
        if active_method == 'dare_merge':
            drop_rate, scaling_lambda = param_value
            output_name = f"{instruct_name}_{thinking_name}_DARE_p{int(drop_rate*100)}_lambda{str(scaling_lambda).replace('.', '')}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.dare_merge_models(
                base_model_path, instruct_model_path, thinking_model_path, 
                drop_rate, scaling_lambda, model_output_path
            )
        
        elif active_method == 'ties_merge':
            top_k, scaling_lambda = param_value
            output_name = f"{instruct_name}_{thinking_name}_TIES_k{top_k}_lambda{str(scaling_lambda).replace('.', '')}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.ties_merge_models(
                base_model_path,
                [instruct_model_path, thinking_model_path], # TIES å¯ä»¥åˆå¹¶å¤šä¸ªæ¨¡å‹
                top_k,
                scaling_lambda,
                model_output_path
            )
        elif active_method == 'sce_merge':
            sparsity_threshold, scaling_lambda ,fusion_weights= param_value
            output_name = f"{instruct_name}_{thinking_name}_SCE_sparsity{sparsity_threshold}_lambda{str(scaling_lambda).replace('.', '')}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.sce_merge_models(
                base_model_path, [instruct_model_path, thinking_model_path], sparsity_threshold, scaling_lambda, model_output_path, fusion_weights
            )
        # ... å…¶ä»–åˆå¹¶æ–¹æ³•çš„é€»è¾‘å¯ä»¥æŒ‰éœ€æ·»åŠ  ...
        elif active_method == 'weighted_average':
            output_name =f"{instruct_name}_{thinking_name}_weighted_avg_{int(param_value*100)}I-{100-int(param_value*100)}T"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.weighted_average_models(instruct_model_path, thinking_model_path, param_value, model_output_path)
        elif active_method == 'surgical_merge':
            output_name = f"{instruct_name}_{thinking_name}_surgical_merge_top_{param_value}pct".replace('.0pct', 'pct')
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.merge_top_k_diff_models(instruct_model_path, thinking_model_path, param_value, model_output_path)
        elif active_method == 'dare_merge_TA':
            drop_rate, scaling_lambda = param_value
            output_name = f"{instruct_name}_{thinking_name}_DARE_TA_p{int(drop_rate*100)}_lambda_1{str(scaling_lambda).replace('.', '')}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.dare_merge_models_TA(
                base_model_path, instruct_model_path, thinking_model_path, 
                drop_rate, scaling_lambda, model_output_path
            )
        elif active_method == 'lore_merge':
            max_iter,  task_weight = param_value
            output_name = f"{instruct_name}_{thinking_name}_LORE_max_iter{max_iter}_lambda{str( task_weight[0]).replace('.', '')}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.lore_merge_models(
                base_model_path, [instruct_model_path, thinking_model_path], task_weight,max_iter,  model_output_path,0.1
            )
        elif active_method == 'ties_merge_ta':
            top_k, weights = param_value
            output_name = f"{instruct_name}_{thinking_name}_TIES_TA_k{top_k}_weight{str(weights[0]).replace('.', '')}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.ties_merge_models_TA(
                base_model_path, [instruct_model_path, thinking_model_path], top_k, weights, model_output_path
            )
        elif active_method == 'twin_merge':
            mask_rate, scaling_lambda = param_value
            output_name = f"{instruct_name}_{thinking_name}_TWIN_mask{int(mask_rate*100)}_lambda{str(scaling_lambda).replace('.', '')}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.twin_merge_models(
                base_model_path, [instruct_model_path, thinking_model_path], mask_rate, scaling_lambda, model_output_path
            )
        elif active_method == 'emr':
            cnt= param_value
            output_name = f"{instruct_name}_{thinking_name}_EMR_cnt{cnt[0]}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.emr_merge_models(
                base_model_path, [instruct_model_path, thinking_model_path], cnt, model_output_path
            )
        elif active_method == 'slerp':
            cnt= param_value
            output_name = f"{instruct_name}_{thinking_name}_slerp{cnt}"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.slerp_merge_models(
                instruct_model_path, thinking_model_path, cnt, model_output_path
            )
        elif active_method == 'top_k_avg_keep_thinking':
            # é€»è¾‘: k%å·®å¼‚æœ€å¤§çš„å‚æ•°å–å¹³å‡ï¼Œå…¶ä½™ä¿ç•™ thinking model çš„å‚æ•°.
            # å®ç°: è°ƒç”¨ merge_top_k_avg_keep_instruct, å®ƒä¿ç•™ç¬¬äºŒä¸ª(instruct/donor)æ¨¡å‹çš„å‚æ•°, æˆ‘ä»¬å°† thinking model ä½œä¸ºç¬¬äºŒä¸ªå‚æ•°ä¼ å…¥.
            output_name = f"{instruct_name}_{thinking_name}_top_k_avg_{param_value}pct_keep_T".replace('.0pct', 'pct')
            merge_func = lambda: model_merger.merge_top_k_avg_keep_instruct(instruct_model_path, thinking_model_path, param_value, model_output_path)


        elif active_method == 'bottom_k_avg_keep_thinking':
            # é€»è¾‘: k%å·®å¼‚æœ€minçš„å‚æ•°å–å¹³å‡ï¼Œå…¶ä½™ä¿ç•™ thinking model çš„å‚æ•°.
            output_name = f"{instruct_name}_{thinking_name}_bottom_k_avg_{param_value}pct_keep_T"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.merge_bottom_k_avg_keep_instruct(instruct_model_path, thinking_model_path, param_value, model_output_path)
        elif active_method == 'bottom_k_avg_keep_instruct':
            # é€»è¾‘: k%å·®å¼‚æœ€minçš„å‚æ•°å–å¹³å‡ï¼Œå…¶ä½™ä¿ç•™ an instruct model çš„å‚æ•°.
            output_name = f"{instruct_name}_{thinking_name}_bottom_k_avg_{param_value}pct_keep_I"
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.merge_bottom_k_avg_keep_instruct(thinking_model_path,instruct_model_path, param_value, model_output_path)
            #æ¢ä¸€ä¸‹ä¼ å‚çš„é¡ºåºåº”è¯¥å°±å¯ä»¥ã€‚
        elif active_method == 'top_k_avg_keep_instruct':
            # é€»è¾‘: k%å·®å¼‚æœ€å¤§çš„å‚æ•°å–å¹³å‡ï¼Œå…¶ä½™ä¿ç•™ an instruct model çš„å‚æ•°.
            # å®ç°: è°ƒç”¨ merge_top_k_avg_keep_base, å®ƒä¿ç•™ç¬¬ä¸€ä¸ª(base)æ¨¡å‹çš„å‚æ•°, æˆ‘ä»¬å°† instruct model ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥.
            output_name = f"{instruct_name}_{thinking_name}_top_k_avg_{param_value}pct_keep_I".replace('.0pct', 'pct')
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.merge_top_k_avg_keep_base(instruct_model_path, thinking_model_path, param_value, model_output_path)

        elif active_method == 'avg_override_top_k_thinking':
            # é€»è¾‘: æ‰€æœ‰å‚æ•°å¹³å‡ï¼Œä½†å·®å¼‚æœ€å¤§çš„k%å‚æ•°æ¢å› thinking model çš„å‚æ•°.
            # å®ç°: è°ƒç”¨ merge_avg_override_top_k_base, å®ƒä¼šç”¨ç¬¬ä¸€ä¸ª(base)æ¨¡å‹çš„å‚æ•°è¦†ç›–.
            # å› æ­¤, æˆ‘ä»¬å¿…é¡»å°† thinking_model_path ä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°ä¼ å…¥æ‰èƒ½å®ç°æ‰€éœ€é€»è¾‘.
            # æ³¨æ„: è¿™ä¹Ÿä¼šå¯¼è‡´ä¿å­˜çš„ tokenizer æ¥è‡ª thinking model.
            output_name = f"{instruct_name}_{thinking_name}_avg_override_top_{param_value}pct_with_T".replace('.0pct', 'pct')
            model_output_path = os.path.join(BASE_MODEL_OUTPUT_DIR, output_name)
            merge_func = lambda: model_merger.merge_avg_override_top_k_base(thinking_model_path, instruct_model_path, param_value, model_output_path)
        if not merge_func:
            print(f"ğŸ¤· Warning: Implementation logic for method '{active_method}' not found. Skipping.")
            continue
        print(f"æ¨¡å‹å°†ä¿å­˜åˆ°: {model_output_path}")
        if not os.path.exists(model_output_path):
            try: 
                merge_func()
                print(f"âœ… æ¨¡å‹ '{output_name}' ç”ŸæˆæˆåŠŸ!")
            except Exception as e: 
                print(f"âŒ æ¨¡å‹ç”Ÿæˆæ—¶å‡ºé”™: {e}. è·³è¿‡æ­¤å‚æ•°ã€‚")
                continue
        else: 
            print("âš ï¸  è­¦å‘Š: æ¨¡å‹ç›®å½•å·²å­˜åœ¨, è·³è¿‡ç”Ÿæˆæ­¥éª¤ã€‚")
            generated_models_info.append({"model_name": output_name, "output_path": model_output_path})
    print("\n" + "="*80 + "\nâœ… All models processed, now generating unified evaluation scripts...\n" + "="*80)

    tasks_string_for_sh = "\n".join([f'    "{task}"' for task in EVAL_TASKS])
    
    all_benchmarks = {data_name.strip() for task in EVAL_TASKS for data_name in task.split(' ')[0].split(',')}
    benchmarks_string_for_py = ",\n".join([f'    "{b}"' for b in sorted(list(all_benchmarks))])

    model_paths_str_list = [f'    "{os.path.relpath(info["output_path"], start=current_run_dir)}"' for info in generated_models_info]
    model_paths_str = "\n".join(model_paths_str_list)
    
    # ã€é‡è¦ä¿®æ”¹ã€‘è®¡ç®—è¯„ä¼°è„šæœ¬åˆ°æ•°æ®é›†æ–‡ä»¶å¤¹çš„ç›¸å¯¹è·¯å¾„
    data_dir_relative_path = os.path.relpath(DATA_DIR_PATH_IN_PROJECT, start=current_run_dir).replace("\\", "/")
    
    sh_content = RUN_EVAL_SH_TEMPLATE.format(
        model_paths_string=model_paths_str, 
        tasks_string=tasks_string_for_sh,
        data_dir_path=data_dir_relative_path
    )
    sh_filepath = os.path.join(current_run_dir, "run_eval_all.sh")
    with open(sh_filepath, 'w', encoding='utf-8') as f: f.write(sh_content)
    os.chmod(sh_filepath, os.stat(sh_filepath).st_mode | stat.S_IEXEC)
    print(f"   -> Generated multi-task evaluation script: {sh_filepath}")

    model_configs = []
    for info in generated_models_info:
        model_name = info["model_name"]
        results_path = os.path.join("outputs", model_name).replace("\\", "/")
        tokenizer_path = os.path.relpath(info["output_path"], start=current_run_dir).replace("\\", "/")
        model_configs.append(f'    {{"model_name": "{model_name}", "results_path": "{results_path}", "tokenizer_path": "{tokenizer_path}"}}')
    model_configs_str = ",\n".join(model_configs)
    
    py_content = CHECK_ACCURACY_PY_TEMPLATE.format(
        generation_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        model_configs_string=model_configs_str,
        benchmarks_string=benchmarks_string_for_py
    )
    py_filepath = os.path.join(current_run_dir, "check_accuracy_all.py")
    with open(py_filepath, 'w', encoding='utf-8') as f: f.write(py_content)
    print(f"   -> Generated multi-task accuracy check script: {py_filepath}")

    print("\n\nğŸ‰ All experiment scripts have been generated successfully!")
    print("Then execute the evaluation: ./run_eval_all.sh")
    print("After evaluation is complete, check the accuracy: python check_accuracy_all.py")


if __name__ == '__main__':
    main()