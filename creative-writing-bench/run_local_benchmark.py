# run_benchmark_final.py (Final Version - Corrected with Dual Response Saving and GPU Selection)
import os
import re
import uuid
import time
import json
import logging
import sys
import gc
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Any, List

# --- å…³é”®ä¾èµ– ---
try:
    import torch
    from transformers import AutoTokenizer
    import openai
    from openai import OpenAI
except ImportError:
    print("é”™è¯¯ï¼šPyTorch, Transformers æˆ– OpenAI æœªå®‰è£…ã€‚è¯·è¿è¡Œ 'pip install torch transformers openai' æ¥å®‰è£…ã€‚")
    sys.exit(1)

import numpy as np
from tqdm import tqdm
from vllm import LLM, SamplingParams

# =====================================================================================
# âš™ï¸ 1. é…ç½®åŒºåŸŸ
# =====================================================================================

# <--- æ–°å¢åŠŸèƒ½: æŒ‡å®šè¦ä½¿ç”¨çš„GPU ---
# è®¾ç½® CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡ã€‚
# ä¾‹å¦‚: "0,1" è¡¨ç¤ºä»…ä½¿ç”¨ GPU 0 å’Œ GPU 1ã€‚
# è®¾ç½®ä¸º None æˆ– "" è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯è§çš„GPU (é»˜è®¤è¡Œä¸º)ã€‚
VISIBLE_GPUS = "6,7" # <--- åœ¨è¿™é‡Œä¿®æ”¹ä¸ºä½ æƒ³è¦çš„GPU IDï¼Œä¾‹å¦‚ "0,1" æˆ– "2"

# MODELS_TO_TEST = ["../../models/Qwen3-4B-Instruct-2507_Qwen3-4B-Thinking-2507_slerp0.8"]
MODELS_TO_TEST = ["../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_20I-80T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_30I-70T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_40I-60T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_50I-50T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_60I-40T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_70I-30T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_80I-20T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_90I-10T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_weighted_avg_100I-0T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_top_k_avg_1pct_keep_I",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_top_k_avg_2pct_keep_I",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_top_k_avg_5pct_keep_I",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_top_k_avg_10pct_keep_I",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_top_k_avg_20pct_keep_I",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_top_k_avg_50pct_keep_I",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_surgical_merge_top_1pct",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_surgical_merge_top_2pct",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_surgical_merge_top_5pct",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_surgical_merge_top_10pct",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_surgical_merge_top_20pct",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_surgical_merge_top_50pct",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.1",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.2",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.3",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.4",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.5",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.6",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.7",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.8",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_slerp0.9",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_avg_override_top_1pct_with_T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_avg_override_top_2pct_with_T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_avg_override_top_5pct_with_T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_avg_override_top_10pct_with_T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_avg_override_top_20pct_with_T",
    "../../models/Qwen3-30B-A3B-Instruct-2507_Qwen3-30B-A3B-Thinking-2507_avg_override_top_50pct_with_T"]

JUDGE_CONFIG = {
    "model_name": "kimi-k2-turbo-preview",
    "base_url": "https://api.moonshot.cn/v1",
    "api_key": "sk-RXvmowS7kvaEcPpH6E2u4soQE5PZ1N02ZnUALsEscG60uygF", # è¯·æ›¿æ¢ä¸ºæ‚¨çš„æœ‰æ•ˆ API Key
}

VLLM_PARAMS = {
    "tensor_parallel_size": 2,
    "gpu_memory_utilization": 0.90,
    "max_model_len": 4096,
    "trust_remote_code": True,
}

SAMPLING_PARAMS = {
    "temperature": 0.7,
    "min_p": 0.1,
    "max_tokens": 4096
}

BENCHMARK_PARAMS = {
    "output_dir": "cw_output",
    "num_threads_judging": 64,
    "iterations": 3,
    "creative_prompts_file": "data/creative_writing_prompts_v3.json",
    "criteria_file": "data/creative_writing_criteria.txt",
    "negative_criteria_file": "data/negative_criteria.txt",
    "judge_prompt_file": "data/creative_writing_judging_prompt.txt",
}

# =====================================================================================
# ğŸ“š 2. è¾…åŠ©å‡½æ•° (Scoringéƒ¨åˆ†å·²æ›¿æ¢ä¸ºåŸç‰ˆé€»è¾‘)
# =====================================================================================

logging.basicConfig(level="INFO", format='%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')

# --- åˆå§‹åŒ– Judge API å®¢æˆ·ç«¯ ---
try:
    judge_client = OpenAI(
        api_key=JUDGE_CONFIG["api_key"],
        base_url=JUDGE_CONFIG["base_url"],
    )
except Exception as e:
    logging.error(f"åˆå§‹åŒ– Judge API å®¢æˆ·ç«¯å¤±è´¥: {e}")
    judge_client = None

def load_data_file(file_path):
    """ç›´æ¥åŠ è½½æ–‡ä»¶ï¼Œå¦‚æœå¤±è´¥åˆ™ç¨‹åºå´©æºƒå¹¶æŠ¥é”™ã€‚"""
    with open(file_path, 'r', encoding='utf-8') as f:
        if file_path.endswith('.json'):
            return json.load(f)
        else:
            return [line.strip() for line in f if line.strip()]

# ========== æ ¸å¿ƒä¿®æ”¹ï¼šä»è¿™é‡Œå¼€å§‹ï¼Œä½¿ç”¨æ‚¨æä¾›çš„åŸç‰ˆ scoring å‡½æ•° ==========

SCORE_RANGE_MAX = 20 # åŸç‰ˆä»£ç ä¸­çš„å…¨å±€å˜é‡

def parse_judge_scores_creative(judge_model_response: str) -> Dict[str, float]:
    """
    åŸç‰ˆè§£æå‡½æ•°: ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä»è‡ªç„¶è¯­è¨€æ–‡æœ¬ä¸­æå–åˆ†æ•°ã€‚
    """
    scores = {}
    score_pattern1 = r'(.*?):\s*(?:Score\s+)?(-?\d+(?:\.\d+)?)'
    score_pattern2 = r'(.*?):\s*\[(-?\d+(?:\.\d+)?)\]'
    
    matches1 = re.findall(score_pattern1, judge_model_response)
    matches2 = re.findall(score_pattern2, judge_model_response)
    
    for matches in [matches1, matches2]:
        for match in matches:
            metric_name = match[0].strip()
            # è¿‡æ»¤æ‰ä¸€äº›éé¢„æœŸçš„åŒ¹é…
            if len(metric_name) > 50 or len(metric_name) < 2:
                continue
            try:
                score = float(match[1])
                if score <= SCORE_RANGE_MAX:
                    scores[metric_name] = score
            except ValueError:
                continue
    return scores

def call_judge_api(prompt: str) -> dict:
    """ä½¿ç”¨ openai åº“è°ƒç”¨æ‰“åˆ†APIï¼Œå¹¶ä½¿ç”¨åŸç‰ˆè§£æå‡½æ•°ã€‚"""
    if not judge_client:
        return {"raw_judge_text": "[ERROR: Judge client not initialized]", "judge_scores": {}}
        
    for attempt in range(10):
        try:
            response = judge_client.chat.completions.create(
                model=JUDGE_CONFIG["model_name"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0,
                max_tokens=1000,
                timeout=240,
            )
            raw_text = response.choices[0].message.content
            # è°ƒç”¨åŸç‰ˆçš„è§£æå‡½æ•°
            scores = parse_judge_scores_creative(raw_text)
            return {"raw_judge_text": raw_text, "judge_scores": scores}
        except Exception as e:
            logging.warning(f"æ‰“åˆ†APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/10): {e.__class__.__name__} - {e}")
            time.sleep(2)
            
    return {"raw_judge_text": "[ERROR: API call failed after multiple retries]", "judge_scores": {}}

# --- åŸç‰ˆè®¡åˆ†é€»è¾‘å‡½æ•° ---

def invert_if_negative_original(metric: str, score: float, negative_criteria: List[str]) -> float:
    """åŸç‰ˆè´Ÿé¢æŒ‡æ ‡åˆ†æ•°ç¿»è½¬é€»è¾‘"""
    # æ³¨æ„ï¼šåŸç‰ˆä»£ç æ£€æŸ¥ metric æ˜¯å¦åœ¨ negative_criteria åˆ—è¡¨ä¸­
    # è€Œä¸æ˜¯åƒæ‚¨è„šæœ¬ä¸­é‚£æ ·æ£€æŸ¥å­å­—ç¬¦ä¸²
    if metric in negative_criteria:
        return 20.0 - score
    return score

def compute_creative_scores_original(tasks: List[Dict[str, Any]], negative_criteria: List[str]) -> float:
    """åŸç‰ˆè®¡ç®—æ‰€æœ‰ä½œå“å¹³å‡åˆ†çš„é€»è¾‘ (0-20åˆ†åˆ¶)"""
    piece_scores = []
    for task_data in tasks:
        # åœ¨æˆ‘ä»¬çš„ç®€åŒ–è„šæœ¬ä¸­ï¼Œæ¯ä¸ªtaskå°±æ˜¯ä¸€ä¸ªä½œå“ï¼Œæ²¡æœ‰results_by_modifierè¿™å±‚
        j_scores = task_data.get("judge_scores", {})
        if not j_scores:
            continue
        
        local_vals = []
        for metric, val in j_scores.items():
            if isinstance(val, (int, float)):
                # æ£€æŸ¥ metric æ˜¯å¦ä¸åŠ è½½çš„è´Ÿé¢æ ‡å‡†å®Œå…¨åŒ¹é…
                # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°åŸå§‹çš„ criterion name æ¥åšè¿™ä¸ªåˆ¤æ–­
                # è¿™æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºè§£æå‡ºçš„ metric name å¯èƒ½ä¸å®Œå…¨ä¸€è‡´
                # æˆ‘ä»¬é‡‡ç”¨ä¸€ç§è¿‘ä¼¼æ–¹æ³•ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•è´Ÿé¢æ ‡å‡†æ˜¯è§£æå‡ºçš„metricçš„å­ä¸²
                is_negative = any(neg_crit in metric for neg_crit in negative_criteria)
                
                # ç®€åŒ–çš„ç¿»è½¬é€»è¾‘
                if is_negative:
                    new_val = 20.0 - val
                else:
                    new_val = val

                if new_val <= SCORE_RANGE_MAX:
                    local_vals.append(new_val)

        if local_vals:
            # åŸç‰ˆé€»è¾‘æ˜¯å…ˆå¯¹æ¯ä¸ªä½œå“çš„æ‰€æœ‰è¯„åˆ†é¡¹å–å¹³å‡
            piece_score = sum(local_vals) / len(local_vals)
            piece_scores.append(piece_score)

    if not piece_scores:
        return 0.0
    # æœ€åå¯¹æ‰€æœ‰ä½œå“çš„å¹³å‡åˆ†å†å–å¹³å‡
    return sum(piece_scores) / len(piece_scores)

def compute_final_scores_original(tasks: List[Dict[str, Any]], negative_criteria: List[str]) -> dict:
    """åŸç‰ˆæœ€ç»ˆè®¡åˆ†å‡½æ•°ï¼Œæ•´åˆæ‰€æœ‰é€»è¾‘"""
    avg_0_20 = compute_creative_scores_original(tasks, negative_criteria)
    eqbench_score = avg_0_20 * 5.0
    eqbench_score = round(eqbench_score, 2)
    
    # æ”¶é›†æ‰€æœ‰è¯¦ç»†åˆ†æ•°ç”¨äºæŠ¥å‘Š
    scores_by_metric = {}
    for task in tasks:
        if task.get("judge_scores"):
            for metric, val in task["judge_scores"].items():
                if isinstance(val, (int, float)):
                    scores_by_metric.setdefault(metric, []).append(val)
    
    detailed_scores_avg = {m: np.mean(v) for m, v in scores_by_metric.items() if v}

    return {
        "overall_score_0_20": round(avg_0_20, 2),
        "eqbench_creative_score": eqbench_score,
        "detailed_scores_raw": detailed_scores_avg
    }

# ========== æ ¸å¿ƒä¿®æ”¹ç»“æŸ ==========

# =====================================================================================
# ğŸš€ 3. ä¸»æ‰§è¡Œæµç¨‹ (é˜¶æ®µ8è¢«æ›¿æ¢)
# =====================================================================================

if __name__ == "__main__":
    # <--- æ–°å¢: åº”ç”¨GPUé…ç½® ---
    # è¿™ä¸ªæ“ä½œå¿…é¡»åœ¨ä»»ä½•CUDAåº“ï¼ˆå¦‚torchæˆ–vLLMï¼‰åˆå§‹åŒ–ä¹‹å‰å®Œæˆã€‚
    if VISIBLE_GPUS:
        os.environ["CUDA_VISIBLE_DEVICES"] = VISIBLE_GPUS
    
    # --- é˜¶æ®µ 1: åŠ è½½æ‰€æœ‰å¿…éœ€æ–‡ä»¶ ---
    logging.info(f"é˜¶æ®µ 1: åŠ è½½ prompts å’Œ criteria æ–‡ä»¶... (å¯è§GPU: {os.environ.get('CUDA_VISIBLE_DEVICES', 'All')})")
    try:
        prompts_data = load_data_file(BENCHMARK_PARAMS['creative_prompts_file'])
        creative_criteria = load_data_file(BENCHMARK_PARAMS['criteria_file'])
        negative_criteria = load_data_file(BENCHMARK_PARAMS['negative_criteria_file'])
        judge_prompt_template = "".join(load_data_file(BENCHMARK_PARAMS['judge_prompt_file']))
    except FileNotFoundError as e:
        logging.error(f"åŠ è½½æ•°æ®æ–‡ä»¶å¤±è´¥: {e}. è¯·ç¡®ä¿ 'data' æ–‡ä»¶å¤¹åŠå…¶ä¸­æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å­˜åœ¨ã€‚")
        sys.exit(1)
    logging.info("æ–‡ä»¶åŠ è½½å®Œæ¯•ã€‚")

    # --- å¯¹æ¯ä¸ªæ¨¡å‹è¿›è¡Œå®Œæ•´çš„è¯„æµ‹å¾ªç¯ ---
    for model_identifier in MODELS_TO_TEST:
        logging.info(f"===== å¼€å§‹è¯„æµ‹æ¨¡å‹: {model_identifier} =====")
        
        logging.info(f"æ­£åœ¨ä¸ºæ¨¡å‹ '{model_identifier}' åŠ è½½ Tokenizer...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(model_identifier, trust_remote_code=True)
        except Exception as e:
            logging.error(f"åŠ è½½ Tokenizer å¤±è´¥: {e}. è·³è¿‡æ­¤æ¨¡å‹ã€‚")
            continue
        logging.info("Tokenizer åŠ è½½æˆåŠŸã€‚")

        logging.info("é˜¶æ®µ 2: å‡†å¤‡ä»»åŠ¡åˆ—è¡¨å’Œåº”ç”¨èŠå¤©æ¨¡æ¿...")
        tasks = []
        prompts_to_generate = []
        # æ³¨æ„ï¼šè¿™é‡Œçš„å¾ªç¯ä¸ºæ¯ä¸ªpromptåˆ›å»ºäº†iterationsæ¬¡ä»»åŠ¡
        for prompt_id, prompt_obj in prompts_data.items():
            for i in range(1, BENCHMARK_PARAMS['iterations'] + 1):
                seed = prompt_obj.get("seed_modifiers", ["default"])[(i - 1) % len(prompt_obj.get("seed_modifiers", ["default"]))]
                raw_prompt = prompt_obj["writing_prompt"].replace("<SEED>", seed)
                messages = [{"role": "user", "content": raw_prompt}]
                final_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                
                task = {
                    "prompt_id": prompt_id, "iteration": i, "base_prompt": prompt_obj["writing_prompt"],
                    "model_response": None,
                    "cleaned_model_response": None, # <--- ä¿®æ”¹ç‚¹ 1: ä¸ºæ¸…ç†åçš„å“åº”æ·»åŠ å ä½ç¬¦
                    "judge_scores": None, "raw_judge_text": None,
                    "response_token_length": None,
                }
                tasks.append(task)
                prompts_to_generate.append(final_prompt)
        logging.info(f"å·²å‡†å¤‡ {len(tasks)} ä¸ªæ ¼å¼åŒ–åçš„ç”Ÿæˆä»»åŠ¡ã€‚")

        logging.info("é˜¶æ®µ 3: åˆå§‹åŒ– vLLM å¼•æ“...")
        llm = LLM(model=model_identifier, **VLLM_PARAMS)
        logging.info("vLLM å¼•æ“åˆå§‹åŒ–æˆåŠŸã€‚")

        logging.info("é˜¶æ®µ 4: å¼€å§‹æ‰¹é‡ç”Ÿæˆæ–‡æœ¬...")
        generated_responses = llm.generate(prompts_to_generate, SamplingParams(**SAMPLING_PARAMS), use_tqdm=True)
        
        # --- è®¡ç®—Tokené•¿åº¦çš„é€»è¾‘ ---
        all_token_lengths = []
        for i, response in enumerate(generated_responses):
            raw_response_text = response.outputs[0].text
            tasks[i]["model_response"] = raw_response_text
            # ä½¿ç”¨åŠ è½½çš„tokenizerè®¡ç®—tokenæ•°é‡å¹¶å­˜å‚¨
            token_count = len(tokenizer.encode(raw_response_text))
            tasks[i]["response_token_length"] = token_count
            all_token_lengths.append(token_count)
            
        # --- è®¡ç®—å¹³å‡Tokené•¿åº¦ ---
        avg_token_length = sum(all_token_lengths) / len(all_token_lengths) if all_token_lengths else 0.0
        logging.info(f"æˆåŠŸç”Ÿæˆäº† {len(generated_responses)} æ¡å“åº”ã€‚å¹³å‡Tokené•¿åº¦: {avg_token_length:.2f}")

        logging.info("é˜¶æ®µ 5: æ¸…ç†GPUèµ„æº...")
        del llm
        gc.collect()
        torch.cuda.empty_cache()
        logging.info("GPUèµ„æºå·²é‡Šæ”¾ã€‚")
        
        logging.info("é˜¶æ®µ 6: å‡†å¤‡æ‰“åˆ†ä»»åŠ¡...")
        tasks_to_judge = []
        judge_prompts = []
        for task in tasks:
            if task["model_response"] and len(task["model_response"].strip()) >= 50:
                response_for_judge = re.sub(r'<think>.*?</think>', '', task["model_response"], flags=re.DOTALL).strip()
                task["cleaned_model_response"] = response_for_judge # <--- ä¿®æ”¹ç‚¹ 2: å°†æ¸…ç†åçš„å“åº”ä¿å­˜å›taskå­—å…¸
                
                if len(response_for_judge) >= 50:
                    judge_prompts.append(judge_prompt_template.format(
                        writing_prompt=task["base_prompt"], 
                        test_model_response=response_for_judge,
                        creative_writing_criteria="\n".join([f"- {c}" for c in creative_criteria]),
                        lower_is_better_criteria=", ".join(negative_criteria)))
                    tasks_to_judge.append(task)
                else:
                    task["judge_scores"], task["raw_judge_text"] = {}, "[SKIPPED - Response too short after cleaning <think> tags]"
            else:
                short_response_info = f"len={len(task['model_response'].strip())}" if task["model_response"] else "failed"
                task["judge_scores"], task["raw_judge_text"] = {}, f"[SKIPPED - Generation too short or failed ({short_response_info})]"
        
        if tasks_to_judge:
            logging.info(f"é˜¶æ®µ 7: å¼€å§‹å¯¹ {len(tasks_to_judge)} ä¸ªæœ‰æ•ˆæ–‡æœ¬è¿›è¡Œå¹¶å‘æ‰“åˆ†...")
            with ThreadPoolExecutor(max_workers=BENCHMARK_PARAMS['num_threads_judging']) as executor:
                judging_results = list(tqdm(executor.map(call_judge_api, judge_prompts), total=len(judge_prompts), desc="Judging"))

            for i, result in enumerate(judging_results):
                tasks_to_judge[i].update(result)
            logging.info("æ‰“åˆ†ä»»åŠ¡å®Œæˆã€‚")
        else:
            logging.info("é˜¶æ®µ 7: æ²¡æœ‰æœ‰æ•ˆçš„ç”Ÿæˆæ–‡æœ¬éœ€è¦æ‰“åˆ†ã€‚")

        # --- é˜¶æ®µ 8: ä½¿ç”¨åŸç‰ˆé€»è¾‘è®¡ç®—æœ€ç»ˆåˆ†æ•° ---
        logging.info("é˜¶æ®µ 8: è®¡ç®—æœ€ç»ˆåˆ†æ•°...")
        final_scores_dict = compute_final_scores_original(tasks_to_judge, negative_criteria)
        logging.info("åˆ†æ•°è®¡ç®—å®Œæˆã€‚")


        # --- é˜¶æ®µ 9: ä¿å­˜ç»“æœå¹¶æ‰“å°æŠ¥å‘Š ---
        logging.info("é˜¶æ®µ 9: ä¿å­˜è¯¦ç»†ç»“æœå¹¶æ‰“å°æ€»ç»“æŠ¥å‘Š...")
        sanitized_name = re.sub(r'[^a-zA-Z0-9_-]+', '_', model_identifier.split('/')[-1])
        output_dir = BENCHMARK_PARAMS['output_dir']
        os.makedirs(output_dir, exist_ok=True)
        results_file = os.path.join(output_dir, f"final_results_{sanitized_name}_{uuid.uuid4().hex[:6]}.json")
        
        final_output = {
            "model_tested": model_identifier, "judge_model": JUDGE_CONFIG["model_name"],
            "run_timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "final_scores": {
                "rubric_score_0_20": final_scores_dict["overall_score_0_20"], 
                "eqbench_score_0_100": final_scores_dict["eqbench_creative_score"],
                "avg_response_token_length": round(avg_token_length, 2),
                "detailed_scores_raw": final_scores_dict["detailed_scores_raw"],
            },
            "individual_results": tasks # <--- ç°åœ¨è¿™é‡Œä¿å­˜çš„tasksåŒ…å«äº†ä¸¤ä¸ªç‰ˆæœ¬çš„response
        }
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=2, ensure_ascii=False)

        logging.info(f"å®Œæ•´ç»“æœå·²ä¿å­˜è‡³: {results_file}")
        print("\n" + "="*60)
        print(f"  æ¨¡å‹è¯„æµ‹å®Œæˆ: {model_identifier}")
        print("-"*60)
        print(f"  Rubric Score (0-20):    {final_scores_dict['overall_score_0_20']:.2f}")
        print(f"  EQ-Bench Score (0-100): {final_scores_dict['eqbench_creative_score']:.2f}")
        print(f"  Avg. Token Length:      {avg_token_length:.2f}")
        print("="*60 + "\n")

    logging.info("æ‰€æœ‰æ¨¡å‹å‡å·²è¯„æµ‹å®Œæ¯•ã€‚")